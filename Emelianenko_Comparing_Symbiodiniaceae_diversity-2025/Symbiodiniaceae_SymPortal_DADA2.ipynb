{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This document covers running SymPortal and DADA2 ASV assignment on the data."
      ],
      "metadata": {
        "id": "FjaK4LmEPHyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data"
      ],
      "metadata": {
        "id": "6XfRdeuefgOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the raw fastq files can be found at the NCBI Sequence Read Archive under the BioProject ID PRJNA1299766."
      ],
      "metadata": {
        "id": "vqzulSIijnvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is organised and managed on the HPC cluster. The output is copied to a local computer for making plots and running statistical tests with R within RStudio."
      ],
      "metadata": {
        "id": "GvguSVVxGy_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "> pwd # print working directory on the OIST HPC cluster\n",
        "/flash/HusnikU/Vera/Symbiodiniaceae\n",
        "\n",
        "> ls -1   # list the files - all the filenames are the same as in the BioProject PRJNA1299766\n",
        "```\n",
        "<details>\n",
        "  <summary>List of all the fastq files [click to expand]</summary>\n",
        "\n",
        "```\n",
        "A1-1a1-ITS2_S179_L001_R1_001.fastq.gz\n",
        "A1-1a1-ITS2_S179_L001_R2_001.fastq.gz\n",
        "A1-1a2-ITS2_S180_L001_R1_001.fastq.gz\n",
        "A1-1a2-ITS2_S180_L001_R2_001.fastq.gz\n",
        "A1-2a1-ITS2_S181_L001_R1_001.fastq.gz\n",
        "A1-2a1-ITS2_S181_L001_R2_001.fastq.gz\n",
        "A1-2a2-ITS2_S182_L001_R1_001.fastq.gz\n",
        "A1-2a2-ITS2_S182_L001_R2_001.fastq.gz\n",
        "A1-5a1-ITS2_S183_L001_R1_001.fastq.gz\n",
        "A1-5a1-ITS2_S183_L001_R2_001.fastq.gz\n",
        "A1-5a2-ITS2_S184_L001_R1_001.fastq.gz\n",
        "A1-5a2-ITS2_S184_L001_R2_001.fastq.gz\n",
        "A1-6a1-ITS2_S185_L001_R1_001.fastq.gz\n",
        "A1-6a1-ITS2_S185_L001_R2_001.fastq.gz\n",
        "A1-6a2-ITS2_S186_L001_R1_001.fastq.gz\n",
        "A1-6a2-ITS2_S186_L001_R2_001.fastq.gz\n",
        "A1_1a1_ITS2_S179_R1_001.fastq.gz\n",
        "A1_1a1_ITS2_S179_R2_001.fastq.gz\n",
        "A1_1a2_ITS2_S180_R1_001.fastq.gz\n",
        "A1_1a2_ITS2_S180_R2_001.fastq.gz\n",
        "A1_2a1_ITS2_S181_R1_001.fastq.gz\n",
        "A1_2a1_ITS2_S181_R2_001.fastq.gz\n",
        "A1_2a2_ITS2_S182_R1_001.fastq.gz\n",
        "A1_2a2_ITS2_S182_R2_001.fastq.gz\n",
        "A1_5a1_ITS2_S183_R1_001.fastq.gz\n",
        "A1_5a1_ITS2_S183_R2_001.fastq.gz\n",
        "A1_5a2_ITS2_S184_R1_001.fastq.gz\n",
        "A1_5a2_ITS2_S184_R2_001.fastq.gz\n",
        "A1_6a1_ITS2_S185_R1_001.fastq.gz\n",
        "A1_6a1_ITS2_S185_R2_001.fastq.gz\n",
        "A1_6a2_ITS2_S186_R1_001.fastq.gz\n",
        "A1_6a2_ITS2_S186_R2_001.fastq.gz\n",
        "A2-2a5-ITS2_S191_L001_R1_001.fastq.gz\n",
        "A2-2a5-ITS2_S191_L001_R2_001.fastq.gz\n",
        "A2-2a6-ITS2_S192_L001_R1_001.fastq.gz\n",
        "A2-2a6-ITS2_S192_L001_R2_001.fastq.gz\n",
        "A2-5a3-ITS2_S187_L001_R1_001.fastq.gz\n",
        "A2-5a3-ITS2_S187_L001_R2_001.fastq.gz\n",
        "A2-5a4-ITS2_S188_L001_R1_001.fastq.gz\n",
        "A2-5a4-ITS2_S188_L001_R2_001.fastq.gz\n",
        "A2-6a3-ITS2_S189_L001_R1_001.fastq.gz\n",
        "A2-6a3-ITS2_S189_L001_R2_001.fastq.gz\n",
        "A2-6a4-ITS2_S190_L001_R1_001.fastq.gz\n",
        "A2-6a4-ITS2_S190_L001_R2_001.fastq.gz\n",
        "A2-7a3-ITS2_S193_L001_R1_001.fastq.gz\n",
        "A2-7a3-ITS2_S193_L001_R2_001.fastq.gz\n",
        "A2-7a4-ITS2_S194_L001_R1_001.fastq.gz\n",
        "A2-7a4-ITS2_S194_L001_R2_001.fastq.gz\n",
        "A2_2a5_ITS2_S191_R1_001.fastq.gz\n",
        "A2_2a5_ITS2_S191_R2_001.fastq.gz\n",
        "A2_2a6_ITS2_S192_R1_001.fastq.gz\n",
        "A2_2a6_ITS2_S192_R2_001.fastq.gz\n",
        "A2_5a3_ITS2_S187_R1_001.fastq.gz\n",
        "A2_5a3_ITS2_S187_R2_001.fastq.gz\n",
        "A2_5a4_ITS2_S188_R1_001.fastq.gz\n",
        "A2_5a4_ITS2_S188_R2_001.fastq.gz\n",
        "A2_6a3_ITS2_S189_R1_001.fastq.gz\n",
        "A2_6a3_ITS2_S189_R2_001.fastq.gz\n",
        "A2_6a4_ITS2_S190_R1_001.fastq.gz\n",
        "A2_6a4_ITS2_S190_R2_001.fastq.gz\n",
        "A2_7a3_ITS2_S193_R1_001.fastq.gz\n",
        "A2_7a3_ITS2_S193_R2_001.fastq.gz\n",
        "A2_7a4_ITS2_S194_R1_001.fastq.gz\n",
        "A2_7a4_ITS2_S194_R2_001.fastq.gz\n",
        "An1-ITS2_S125_L001_R1_001.fastq.gz\n",
        "An1-ITS2_S125_L001_R2_001.fastq.gz\n",
        "An1_ITS2_S125_R1_001.fastq.gz\n",
        "An1_ITS2_S125_R2_001.fastq.gz\n",
        "An2-ITS2_S126_L001_R1_001.fastq.gz\n",
        "An2-ITS2_S126_L001_R2_001.fastq.gz\n",
        "An2_ITS2_S126_R1_001.fastq.gz\n",
        "An2_ITS2_S126_R2_001.fastq.gz\n",
        "C1-1-ITS2_S107_L001_R1_001.fastq.gz\n",
        "C1-1-ITS2_S107_L001_R2_001.fastq.gz\n",
        "C1-12-corr_ITS2_S112_L001_R1_001.fastq.gz\n",
        "C1-12_corr_ITS2_S112_L001_R2_001.fastq.gz\n",
        "C1-14-ITS2_S111_L001_R1_001.fastq.gz\n",
        "C1-14-ITS2_S111_L001_R2_001.fastq.gz\n",
        "C1-1_S71_L001_R1_001.fastq.gz\n",
        "C1-1_S71_L001_R2_001.fastq.gz\n",
        "C1-2_corr_ITS2_S114_L001_R1_001.fastq.gz\n",
        "C1-2_corr_ITS2_S114_L001_R2_001.fastq.gz\n",
        "C1-3-ITS2_S108_L001_R1_001.fastq.gz\n",
        "C1-3-ITS2_S108_L001_R2_001.fastq.gz\n",
        "C1-5-ITS2_S109_L001_R1_001.fastq.gz\n",
        "C1-5-ITS2_S109_L001_R2_001.fastq.gz\n",
        "C1-6-ITS2_S113_L001_R1_001.fastq.gz\n",
        "C1-6-ITS2_S113_L001_R2_001.fastq.gz\n",
        "C1-9-ITS2_S110_L001_R1_001.fastq.gz\n",
        "C1-9-ITS2_S110_L001_R2_001.fastq.gz\n",
        "C1_12_corr_ITS2_S112_R1_001.fastq.gz\n",
        "C1_12_corr_ITS2_S112_R2_001.fastq.gz\n",
        "C1_14_ITS2_S111_R1_001.fastq.gz\n",
        "C1_14_ITS2_S111_R2_001.fastq.gz\n",
        "C1_1_ITS2_S107_R1_001.fastq.gz\n",
        "C1_1_ITS2_S107_R2_001.fastq.gz\n",
        "C1_2_corr_ITS2_S114_R1_001.fastq.gz\n",
        "C1_2_corr_ITS2_S114_R2_001.fastq.gz\n",
        "C1_3_ITS2_S108_R1_001.fastq.gz\n",
        "C1_3_ITS2_S108_R2_001.fastq.gz\n",
        "C1_5_ITS2_S109_R1_001.fastq.gz\n",
        "C1_5_ITS2_S109_R2_001.fastq.gz\n",
        "C1_6_ITS2_S113_R1_001.fastq.gz\n",
        "C1_6_ITS2_S113_R2_001.fastq.gz\n",
        "C1_9_ITS2_S110_R1_001.fastq.gz\n",
        "C1_9_ITS2_S110_R2_001.fastq.gz\n",
        "C2-1-ITS2_S115_L001_R1_001.fastq.gz\n",
        "C2-1-ITS2_S115_L001_R2_001.fastq.gz\n",
        "C2-14b-ITS2_S121_L001_R1_001.fastq.gz\n",
        "C2-14b-ITS2_S121_L001_R2_001.fastq.gz\n",
        "C2-16-ITS2_S122_L001_R1_001.fastq.gz\n",
        "C2-16-ITS2_S122_L001_R2_001.fastq.gz\n",
        "C2-18-ITS2_S123_L001_R1_001.fastq.gz\n",
        "C2-18-ITS2_S123_L001_R2_001.fastq.gz\n",
        "C2-19-ITS2_S124_L001_R1_001.fastq.gz\n",
        "C2-19-ITS2_S124_L001_R2_001.fastq.gz\n",
        "C2-2-ITS2_S120_L001_R1_001.fastq.gz\n",
        "C2-2-ITS2_S120_L001_R2_001.fastq.gz\n",
        "C2-3-ITS2_S116_L001_R1_001.fastq.gz\n",
        "C2-3-ITS2_S116_L001_R2_001.fastq.gz\n",
        "C2-6-ITS2_S117_L001_R1_001.fastq.gz\n",
        "C2-6-ITS2_S117_L001_R2_001.fastq.gz\n",
        "C2-7-ITS2_S118_L001_R1_001.fastq.gz\n",
        "C2-7-ITS2_S118_L001_R2_001.fastq.gz\n",
        "C2-9-ITS2_S119_L001_R1_001.fastq.gz\n",
        "C2-9-ITS2_S119_L001_R2_001.fastq.gz\n",
        "C2_14b_ITS2_S121_R1_001.fastq.gz\n",
        "C2_14b_ITS2_S121_R2_001.fastq.gz\n",
        "C2_16_ITS2_S122_R1_001.fastq.gz\n",
        "C2_16_ITS2_S122_R2_001.fastq.gz\n",
        "C2_18_ITS2_S123_R1_001.fastq.gz\n",
        "C2_18_ITS2_S123_R2_001.fastq.gz\n",
        "C2_19_ITS2_S124_R1_001.fastq.gz\n",
        "C2_19_ITS2_S124_R2_001.fastq.gz\n",
        "C2_1_ITS2_S115_R1_001.fastq.gz\n",
        "C2_1_ITS2_S115_R2_001.fastq.gz\n",
        "C2_2_ITS2_S120_R1_001.fastq.gz\n",
        "C2_2_ITS2_S120_R2_001.fastq.gz\n",
        "C2_3_ITS2_S116_R1_001.fastq.gz\n",
        "C2_3_ITS2_S116_R2_001.fastq.gz\n",
        "C2_6_ITS2_S117_R1_001.fastq.gz\n",
        "C2_6_ITS2_S117_R2_001.fastq.gz\n",
        "C2_7_ITS2_S118_R1_001.fastq.gz\n",
        "C2_7_ITS2_S118_R2_001.fastq.gz\n",
        "C2_9_ITS2_S119_R1_001.fastq.gz\n",
        "C2_9_ITS2_S119_R2_001.fastq.gz\n",
        "F1-10-ITS2_S141_L001_R1_001.fastq.gz\n",
        "F1-10-ITS2_S141_L001_R2_001.fastq.gz\n",
        "F1-12-ITS2_S149_L001_R1_001.fastq.gz\n",
        "F1-12-ITS2_S149_L001_R2_001.fastq.gz\n",
        "F1-13-ITS2_S150_L001_R1_001.fastq.gz\n",
        "F1-13-ITS2_S150_L001_R2_001.fastq.gz\n",
        "F1-14-ITS2_S151_L001_R1_001.fastq.gz\n",
        "F1-14-ITS2_S151_L001_R2_001.fastq.gz\n",
        "F1-2-ITS2_S136_L001_R1_001.fastq.gz\n",
        "F1-2-ITS2_S136_L001_R2_001.fastq.gz\n",
        "F1-3-ITS2_S137_L001_R1_001.fastq.gz\n",
        "F1-3-ITS2_S137_L001_R2_001.fastq.gz\n",
        "F1-4-ITS2_S138_L001_R1_001.fastq.gz\n",
        "F1-4-ITS2_S138_L001_R2_001.fastq.gz\n",
        "F1-5-ITS2_S139_L001_R1_001.fastq.gz\n",
        "F1-5-ITS2_S139_L001_R2_001.fastq.gz\n",
        "F1-7-ITS2_S140_L001_R1_001.fastq.gz\n",
        "F1-7-ITS2_S140_L001_R2_001.fastq.gz\n",
        "F1-9-ITS2_S148_L001_R1_001.fastq.gz\n",
        "F1-9-ITS2_S148_L001_R2_001.fastq.gz\n",
        "F1_10_ITS2_S141_R1_001.fastq.gz\n",
        "F1_10_ITS2_S141_R2_001.fastq.gz\n",
        "F1_12_ITS2_S149_R1_001.fastq.gz\n",
        "F1_12_ITS2_S149_R2_001.fastq.gz\n",
        "F1_13_ITS2_S150_R1_001.fastq.gz\n",
        "F1_13_ITS2_S150_R2_001.fastq.gz\n",
        "F1_14_ITS2_S151_R1_001.fastq.gz\n",
        "F1_14_ITS2_S151_R2_001.fastq.gz\n",
        "F1_2_ITS2_S136_R1_001.fastq.gz\n",
        "F1_2_ITS2_S136_R2_001.fastq.gz\n",
        "F1_3_ITS2_S137_R1_001.fastq.gz\n",
        "F1_3_ITS2_S137_R2_001.fastq.gz\n",
        "F1_4_ITS2_S138_R1_001.fastq.gz\n",
        "F1_4_ITS2_S138_R2_001.fastq.gz\n",
        "F1_5_ITS2_S139_R1_001.fastq.gz\n",
        "F1_5_ITS2_S139_R2_001.fastq.gz\n",
        "F1_7_ITS2_S140_R1_001.fastq.gz\n",
        "F1_7_ITS2_S140_R2_001.fastq.gz\n",
        "F1_9_ITS2_S148_R1_001.fastq.gz\n",
        "F1_9_ITS2_S148_R2_001.fastq.gz\n",
        "F2-1-ITS2_S142_L001_R1_001.fastq.gz\n",
        "F2-1-ITS2_S142_L001_R2_001.fastq.gz\n",
        "F2-2-ITS2_S143_L001_R1_001.fastq.gz\n",
        "F2-2-ITS2_S143_L001_R2_001.fastq.gz\n",
        "F2-21-ITS2_S152_L001_R1_001.fastq.gz\n",
        "F2-21-ITS2_S152_L001_R2_001.fastq.gz\n",
        "F2-22-ITS2_S153_L001_R1_001.fastq.gz\n",
        "F2-22-ITS2_S153_L001_R2_001.fastq.gz\n",
        "F2-29-ITS2_S154_L001_R1_001.fastq.gz\n",
        "F2-29-ITS2_S154_L001_R2_001.fastq.gz\n",
        "F2-3-ITS2_S144_L001_R1_001.fastq.gz\n",
        "F2-3-ITS2_S144_L001_R2_001.fastq.gz\n",
        "F2-30-ITS2_S146_L001_R1_001.fastq.gz\n",
        "F2-30-ITS2_S146_L001_R2_001.fastq.gz\n",
        "F2-31-ITS2_S147_L001_R1_001.fastq.gz\n",
        "F2-31-ITS2_S147_L001_R2_001.fastq.gz\n",
        "F2-4-ITS2_S145_L001_R1_001.fastq.gz\n",
        "F2-4-ITS2_S145_L001_R2_001.fastq.gz\n",
        "F2-5_S72_L001_R1_001.fastq.gz\n",
        "F2-5_S72_L001_R2_001.fastq.gz\n",
        "F2-6_S73_L001_R1_001.fastq.gz\n",
        "F2-6_S73_L001_R2_001.fastq.gz\n",
        "F2_1_ITS2_S142_R1_001.fastq.gz\n",
        "F2_1_ITS2_S142_R2_001.fastq.gz\n",
        "F2_21_ITS2_S152_R1_001.fastq.gz\n",
        "F2_21_ITS2_S152_R2_001.fastq.gz\n",
        "F2_22_ITS2_S153_R1_001.fastq.gz\n",
        "F2_22_ITS2_S153_R2_001.fastq.gz\n",
        "F2_29_ITS2_S154_R1_001.fastq.gz\n",
        "F2_29_ITS2_S154_R2_001.fastq.gz\n",
        "F2_2_ITS2_S143_R1_001.fastq.gz\n",
        "F2_2_ITS2_S143_R2_001.fastq.gz\n",
        "F2_30_ITS2_S146_R1_001.fastq.gz\n",
        "F2_30_ITS2_S146_R2_001.fastq.gz\n",
        "F2_31_ITS2_S147_R1_001.fastq.gz\n",
        "F2_31_ITS2_S147_R2_001.fastq.gz\n",
        "F2_3_ITS2_S144_R1_001.fastq.gz\n",
        "F2_3_ITS2_S144_R2_001.fastq.gz\n",
        "F2_4_ITS2_S145_R1_001.fastq.gz\n",
        "F2_4_ITS2_S145_R2_001.fastq.gz\n",
        "S1-5a1-ITS2_S155_L001_R1_001.fastq.gz\n",
        "S1-5a1-ITS2_S155_L001_R2_001.fastq.gz\n",
        "S1-5a2-ITS2_S156_L001_R1_001.fastq.gz\n",
        "S1-5a2-ITS2_S156_L001_R2_001.fastq.gz\n",
        "S1-6a1-ITS2_S157_L001_R1_001.fastq.gz\n",
        "S1-6a1-ITS2_S157_L001_R2_001.fastq.gz\n",
        "S1-6a2-ITS2_S158_L001_R1_001.fastq.gz\n",
        "S1-6a2-ITS2_S158_L001_R2_001.fastq.gz\n",
        "S1-7a1-ITS2_S159_L001_R1_001.fastq.gz\n",
        "S1-7a1-ITS2_S159_L001_R2_001.fastq.gz\n",
        "S1-7a2-ITS2_S160_L001_R1_001.fastq.gz\n",
        "S1-7a2-ITS2_S160_L001_R2_001.fastq.gz\n",
        "S1-8a3-ITS2_S161_L001_R1_001.fastq.gz\n",
        "S1-8a3-ITS2_S161_L001_R2_001.fastq.gz\n",
        "S1-8a4-ITS2_S162_L001_R1_001.fastq.gz\n",
        "S1-8a4-ITS2_S162_L001_R2_001.fastq.gz\n",
        "S1_5a1_ITS2_S155_R1_001.fastq.gz\n",
        "S1_5a1_ITS2_S155_R2_001.fastq.gz\n",
        "S1_5a2_ITS2_S156_R1_001.fastq.gz\n",
        "S1_5a2_ITS2_S156_R2_001.fastq.gz\n",
        "S1_6a1_ITS2_S157_R1_001.fastq.gz\n",
        "S1_6a1_ITS2_S157_R2_001.fastq.gz\n",
        "S1_6a2_ITS2_S158_R1_001.fastq.gz\n",
        "S1_6a2_ITS2_S158_R2_001.fastq.gz\n",
        "S1_7a1_ITS2_S159_R1_001.fastq.gz\n",
        "S1_7a1_ITS2_S159_R2_001.fastq.gz\n",
        "S1_7a2_ITS2_S160_R1_001.fastq.gz\n",
        "S1_7a2_ITS2_S160_R2_001.fastq.gz\n",
        "S1_8a3_ITS2_S161_R1_001.fastq.gz\n",
        "S1_8a3_ITS2_S161_R2_001.fastq.gz\n",
        "S1_8a4_ITS2_S162_R1_001.fastq.gz\n",
        "S1_8a4_ITS2_S162_R2_001.fastq.gz\n",
        "S2-4a3-ITS2_S163_L001_R1_001.fastq.gz\n",
        "S2-4a3-ITS2_S163_L001_R2_001.fastq.gz\n",
        "S2-4a4-ITS2_S164_L001_R1_001.fastq.gz\n",
        "S2-4a4-ITS2_S164_L001_R2_001.fastq.gz\n",
        "S2-5a1-ITS2_S165_L001_R1_001.fastq.gz\n",
        "S2-5a1-ITS2_S165_L001_R2_001.fastq.gz\n",
        "S2-5a2-ITS2_S166_L001_R1_001.fastq.gz\n",
        "S2-5a2-ITS2_S166_L001_R2_001.fastq.gz\n",
        "S2-6a1-ITS2_S167_L001_R1_001.fastq.gz\n",
        "S2-6a1-ITS2_S167_L001_R2_001.fastq.gz\n",
        "S2-6a2-ITS2_S168_L001_R1_001.fastq.gz\n",
        "S2-6a2-ITS2_S168_L001_R2_001.fastq.gz\n",
        "S2-9a1-ITS2_S169_L001_R1_001.fastq.gz\n",
        "S2-9a1-ITS2_S169_L001_R2_001.fastq.gz\n",
        "S2-9a2-ITS2_S170_L001_R1_001.fastq.gz\n",
        "S2-9a2-ITS2_S170_L001_R2_001.fastq.gz\n",
        "S2_4a3_ITS2_S163_R1_001.fastq.gz\n",
        "S2_4a3_ITS2_S163_R2_001.fastq.gz\n",
        "S2_4a4_ITS2_S164_R1_001.fastq.gz\n",
        "S2_4a4_ITS2_S164_R2_001.fastq.gz\n",
        "S2_5a1_ITS2_S165_R1_001.fastq.gz\n",
        "S2_5a1_ITS2_S165_R2_001.fastq.gz\n",
        "S2_5a2_ITS2_S166_R1_001.fastq.gz\n",
        "S2_5a2_ITS2_S166_R2_001.fastq.gz\n",
        "S2_6a1_ITS2_S167_R1_001.fastq.gz\n",
        "S2_6a1_ITS2_S167_R2_001.fastq.gz\n",
        "S2_6a2_ITS2_S168_R1_001.fastq.gz\n",
        "S2_6a2_ITS2_S168_R2_001.fastq.gz\n",
        "S2_9a1_ITS2_S169_R1_001.fastq.gz\n",
        "S2_9a1_ITS2_S169_R2_001.fastq.gz\n",
        "S2_9a2_ITS2_S170_R1_001.fastq.gz\n",
        "S2_9a2_ITS2_S170_R2_001.fastq.gz\n",
        "TN1a-ITS2_S127_L001_R1_001.fastq.gz\n",
        "TN1a-ITS2_S127_L001_R2_001.fastq.gz\n",
        "TN1a_ITS2_S127_R1_001.fastq.gz\n",
        "TN1a_ITS2_S127_R2_001.fastq.gz\n",
        "TN2a-ITS2_S128_L001_R1_001.fastq.gz\n",
        "TN2a-ITS2_S128_L001_R2_001.fastq.gz\n",
        "TN2a_ITS2_S128_R1_001.fastq.gz\n",
        "TN2a_ITS2_S128_R2_001.fastq.gz\n",
        "TN3a-ITS2_S129_L001_R1_001.fastq.gz\n",
        "TN3a-ITS2_S129_L001_R2_001.fastq.gz\n",
        "TN3a_ITS2_S129_R1_001.fastq.gz\n",
        "TN3a_ITS2_S129_R2_001.fastq.gz\n",
        "TN4a-ITS2_S130_L001_R1_001.fastq.gz\n",
        "TN4a-ITS2_S130_L001_R2_001.fastq.gz\n",
        "TN4a_ITS2_S130_R1_001.fastq.gz\n",
        "TN4a_ITS2_S130_R2_001.fastq.gz\n",
        "TN5a-ITS2_S131_L001_R1_001.fastq.gz\n",
        "TN5a-ITS2_S131_L001_R2_001.fastq.gz\n",
        "TN5a_ITS2_S131_R1_001.fastq.gz\n",
        "TN5a_ITS2_S131_R2_001.fastq.gz\n",
        "TN6a-ITS2_S132_L001_R1_001.fastq.gz\n",
        "TN6a-ITS2_S132_L001_R2_001.fastq.gz\n",
        "TN6a_ITS2_S132_R1_001.fastq.gz\n",
        "TN6a_ITS2_S132_R2_001.fastq.gz\n",
        "TN7a-ITS2_S133_L001_R1_001.fastq.gz\n",
        "TN7a-ITS2_S133_L001_R2_001.fastq.gz\n",
        "TN7a_ITS2_S133_R1_001.fastq.gz\n",
        "TN7a_ITS2_S133_R2_001.fastq.gz\n",
        "TN8a-ITS2_S134_L001_R1_001.fastq.gz\n",
        "TN8a-ITS2_S134_L001_R2_001.fastq.gz\n",
        "TN8a_ITS2_S134_R1_001.fastq.gz\n",
        "TN8a_ITS2_S134_R2_001.fastq.gz\n",
        "TN9a-ITS2_S135_L001_R1_001.fastq.gz\n",
        "TN9a-ITS2_S135_L001_R2_001.fastq.gz\n",
        "TN9a_ITS2_S135_R1_001.fastq.gz\n",
        "TN9a_ITS2_S135_R2_001.fastq.gz\n",
        "W1-1-ITS2_S171_L001_R1_001.fastq.gz\n",
        "W1-1-ITS2_S171_L001_R2_001.fastq.gz\n",
        "W1-2-ITS2_S172_L001_R1_001.fastq.gz\n",
        "W1-2-ITS2_S172_L001_R2_001.fastq.gz\n",
        "W1-3-ITS2_S173_L001_R1_001.fastq.gz\n",
        "W1-3-ITS2_S173_L001_R2_001.fastq.gz\n",
        "W1-4-ITS2_S174_L001_R1_001.fastq.gz\n",
        "W1-4-ITS2_S174_L001_R2_001.fastq.gz\n",
        "W1_1_ITS2_S171_R1_001.fastq.gz\n",
        "W1_1_ITS2_S171_R2_001.fastq.gz\n",
        "W1_2_ITS2_S172_R1_001.fastq.gz\n",
        "W1_2_ITS2_S172_R2_001.fastq.gz\n",
        "W1_3_ITS2_S173_R1_001.fastq.gz\n",
        "W1_3_ITS2_S173_R2_001.fastq.gz\n",
        "W1_4_ITS2_S174_R1_001.fastq.gz\n",
        "W1_4_ITS2_S174_R2_001.fastq.gz\n",
        "W2-5-ITS2_S175_L001_R1_001.fastq.gz\n",
        "W2-5-ITS2_S175_L001_R2_001.fastq.gz\n",
        "W2-6-ITS2_S176_L001_R1_001.fastq.gz\n",
        "W2-6-ITS2_S176_L001_R2_001.fastq.gz\n",
        "W2-7-ITS2_S177_L001_R1_001.fastq.gz\n",
        "W2-7-ITS2_S177_L001_R2_001.fastq.gz\n",
        "W2-8-ITS2_S178_L001_R1_001.fastq.gz\n",
        "W2-8-ITS2_S178_L001_R2_001.fastq.gz\n",
        "W2_5_ITS2_S175_R1_001.fastq.gz\n",
        "W2_5_ITS2_S175_R2_001.fastq.gz\n",
        "W2_6_ITS2_S176_R1_001.fastq.gz\n",
        "W2_6_ITS2_S176_R2_001.fastq.gz\n",
        "W2_7_ITS2_S177_R1_001.fastq.gz\n",
        "W2_7_ITS2_S177_R2_001.fastq.gz\n",
        "W2_8_ITS2_S178_R1_001.fastq.gz\n",
        "W2_8_ITS2_S178_R2_001.fastq.gz\n",
        "```\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "9O0wAAallSPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary> Naming convention </summary>\n",
        "Each collected sample type was initially designated with a letter as following:\n",
        " - A - macroalgae\n",
        " - S - sediment (sand)\n",
        " - C - corals\n",
        " - F - foraminifera\n",
        " - An - sea anemone\n",
        " - TN - Tridacna (at the time of samole collection it was believed it's T. noae hence the N)\n",
        "  - W - water\n",
        "\n",
        "The first number after the letter means collection site. It is absent in Tridacna as Tridacna were purchased from the fishermen and their collection site is slighly different. So, all samples that start with A1 are macroalge from the first site, all the samples that start from F2 are foraminifera from the second site.\n",
        "\n",
        "The second number, after dash / underscore / period means the replica of the sample. For example, W1-4 means \"fourth water sample from the first site\", TN9 means \"ninth Tridacna\". In all the samples except water, the numbers start from 1 in each site. For water samples, only 4 samples were collected per site, so samples \"W2-1\" never existed. With other sample types, missed numbers mean that the sample was collected but not sequenced.\n",
        "\n",
        "The small letter a plus number designates subsample or aliquote - things that come from the same tube of sand / bag of algae / Tridacna individual, but were put in separate eppendorfs and treated for DNA extraction separately. For Tridacna, TN1b and TN1c were never extracted; for algal and sediment samples, DNA was extracted from several aliquots but only two were chosen to be sequenced.\n",
        "</details>"
      ],
      "metadata": {
        "id": "SK9fwuQ_emIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate the files that come from the same sample, but from different Illumina runs. All the samples except F2-5, F2-6 were sequenced first on one Illumina run and then the same samples were sequenced on the second Illumina run due to high amount of low-quality data during the first run. The third run included samples F2-5, F2-6, and C1-1 (resequenced again). Also, the samples of algal and sediment were processed as following: 50 mL of sediment or 50 g of algae were ground using the Mortar Mill RM200 for 10 minutes. Total DNA was extracted from two âˆ¼0.5 ml subsamples of the ground mass, which were processed separately until data analysis. So each of the first sequencing runs contains data from two aliquots of each of the sediment and algal samples.\n",
        "\n",
        "Prepare the data and separate into three folders for each of the sequencing runs\n",
        "```\n",
        "> mkdir raw_seq\n",
        "> mkdir raw_seq/run1 raw_seq/run2 raw_seq/run3\n",
        "> mkdir raw_seq/run1_2\n",
        "> mv *fastq.gz  raw_seq\n",
        "# rename the files for C-12 and C1-2\n",
        "> mv C1-2_corr_ITS2_S114_L001_R1_001.fastq.gz C1-2-corr-ITS2_S114_L001_R1_001.fastq.gz\n",
        "> mv C1-12_corr_ITS2_S112_L001_R2_001.fastq.gz C1-12-corr-ITS2_S112_L001_R2_001.fastq.gz\n",
        "> mv C1-12-corr_ITS2_S112_L001_R1_001.fastq.gz C1-12-corr-ITS2_S112_L001_R1_001.fastq.gz\n",
        "> mv C1-2_corr_ITS2_S114_L001_R2_001.fastq.gz C1-2-corr-ITS2_S114_L001_R2_001.fastq.gz\n",
        "# move all the files with dash to one folder\n",
        "> mv raw_seq/*-ITS2* raw_seq/run2\n",
        "> mv raw_seq/*_ITS2* raw_seq/run1\n",
        "> mv raw_seq/*fastq.gz raw_seq/run3\n",
        "\n",
        "```\n",
        "Make and run a slurm script to concatenate the files from run1 and run2 (file `concat.sh`).\n",
        "\n",
        "<details>\n",
        "  <summary> slurm script description </summary>\n",
        "Slurm is internal OIST's HPC cluster scheduling system. All the options starting with ` #SBATCH` and following the word \"module\" are specific to this internal slurm system. The actual usiversal bash code starts after the row with \"module\".\n",
        "\n",
        "The slurm script is create in text editor and either copied to HPC cluster as a file or is edited with text editor such as `nano` on the cluster itself. To run the slurm script at OIST's HPC cluster, the user uses command `sbatch script.sh`, and the script is run when the scheduler can allocate the resources. Some commands can also be run with interactive jobs (for example with `srun -p short -t 1:00:00 --mem=20G -c 8 --pty bash`).\n",
        "\n",
        "In this document, all the scripts containing #SBATCH lines are run with slurm on the HPC cluster with `sbatch script.sh`; some of the smaller scripts are run within interactive jobs. These commands for editing and running scripts are not provided explicitly after this section. Small tasks such as `ls` or `cd` are performed without the slurm job.\n",
        "</details>\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=concat\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=10G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=10\n",
        "#SBATCH --output=concat-%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module purge # make sure no modules are loaded\n",
        "\n",
        "path=\"/flash/HusnikU/Vera/Symbiodiniaceae/raw_seq\"\n",
        "\n",
        "for file in ${path}/run2/*R1_001.fastq.gz; do\n",
        "  sample_id_run2=$(basename ${file%_L001_R1_001.fastq.gz}) #for example  S1-8a4-ITS2_S162\n",
        "  sample_id_run1=${sample_id_run2//-/_} # change all dashes to underscores\n",
        "\n",
        "  cat ${path}/run2/${sample_id_run2}_L001_R1_001.fastq.gz ${path}/run1/${sample_id_run1}_R1_001.fastq.gz > ${path}/run1_2/${sample_id_run1}_run1_2_R1_001.fastq.gz # Read1\n",
        "\n",
        "  cat ${path}/run2/${sample_id_run2}_L001_R2_001.fastq.gz ${path}/run1/${sample_id_run1}_R2_001.fastq.gz > ${path}/run1_2/${sample_id_run1}_run1_2_R2_001.fastq.gz # Read2\n",
        "done\n",
        "\n",
        "```\n",
        "\n",
        "Concatenate the file for C1-1\n",
        "\n",
        "```\n",
        "> cat raw_seq/run1_2/C1_1_ITS2_S107_run1_2_R1_001.fastq.gz raw_seq/run3/C1-1_S71_L001_R1_001.fastq.gz > raw_seq/run1_2/C1_1_ITS2_S107_run1_2_3_R1_001.fastq.gz\n",
        "> cat raw_seq/run1_2/C1_1_ITS2_S107_run1_2_R2_001.fastq.gz raw_seq/run3/C1-1_S71_L001_R2_001.fastq.gz > raw_seq/run1_2/C1_1_ITS2_S107_run1_2_3_R2_001.fastq.gz\n",
        "> rm raw_seq/run1_2/C1_1_ITS2_S107_run1_2_R2_001.fastq.gz\n",
        "> rm raw_seq/run1_2/C1_1_ITS2_S107_run1_2_R1_001.fastq.gz\n",
        "> cp raw_seq/run3/F* -t raw_seq/run1_2\n",
        "```\n",
        "\n",
        "Now all the files concatenated between the runs are in `raw_seq/run1_2`."
      ],
      "metadata": {
        "id": "rzMfY5s_mKD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory for environmental files of algae and sediment\n",
        "\n",
        "```\n",
        "> mkdir raw_seq/env\n",
        "> mv raw_seq/run1_2/A1* raw_seq/run1_2/A2* raw_seq/run1_2/S1* raw_seq/run1_2/S2* -t raw_seq/env\n",
        "# 64 files moved\n",
        "> cd raw_seq/env\n",
        "```\n",
        "Create a directory for concatenated data of algal and sediment samples\n",
        "\n",
        "```\n",
        "> mkdir ../../concat\n",
        "```\n",
        "\n",
        "Concatenate the files with sequencing data from different aliquots manually with concat_env.sh\n",
        "\n",
        "<details>\n",
        "  <summary>concat_env.sh [click to expand]</summary>\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=concat_env\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=10G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=10\n",
        "#SBATCH --output=concat-env%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module purge\n",
        "\n",
        "cat A1_1a1_ITS2_S179_run1_2_R1_001.fastq.gz A1_1a2_ITS2_S180_run1_2_R1_001.fastq.gz > ../../concat/A1_1_ITS2_S180_run1_2_R1_001.fastq.gz\n",
        "cat A1_1a1_ITS2_S179_run1_2_R2_001.fastq.gz A1_1a2_ITS2_S180_run1_2_R2_001.fastq.gz > ../../concat/A1_1_ITS2_S180_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A1_2a1_ITS2_S181_run1_2_R1_001.fastq.gz A1_2a2_ITS2_S182_run1_2_R1_001.fastq.gz > ../../concat/A1_2_ITS2_S182_run1_2_R1_001.fastq.gz\n",
        "cat A1_2a1_ITS2_S181_run1_2_R2_001.fastq.gz A1_2a2_ITS2_S182_run1_2_R2_001.fastq.gz > ../../concat/A1_2_ITS2_S182_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A1_5a1_ITS2_S183_run1_2_R1_001.fastq.gz A1_5a2_ITS2_S184_run1_2_R1_001.fastq.gz > ../../concat/A1_5_ITS2_S184_run1_2_R1_001.fastq.gz\n",
        "cat A1_5a1_ITS2_S183_run1_2_R2_001.fastq.gz A1_5a2_ITS2_S184_run1_2_R2_001.fastq.gz > ../../concat/A1_5_ITS2_S184_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A1_6a1_ITS2_S185_run1_2_R1_001.fastq.gz A1_6a2_ITS2_S186_run1_2_R1_001.fastq.gz > ../../concat/A1_6_ITS2_S186_run1_2_R1_001.fastq.gz\n",
        "cat A1_6a1_ITS2_S185_run1_2_R2_001.fastq.gz A1_6a2_ITS2_S186_run1_2_R2_001.fastq.gz > ../../concat/A1_6_ITS2_S186_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A2_2a5_ITS2_S191_run1_2_R1_001.fastq.gz A2_2a6_ITS2_S192_run1_2_R1_001.fastq.gz > ../../concat/A2_2_ITS2_S192_run1_2_R1_001.fastq.gz\n",
        "cat A2_2a5_ITS2_S191_run1_2_R2_001.fastq.gz A2_2a6_ITS2_S192_run1_2_R2_001.fastq.gz > ../../concat/A2_2_ITS2_S192_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A2_5a3_ITS2_S187_run1_2_R1_001.fastq.gz A2_5a4_ITS2_S188_run1_2_R1_001.fastq.gz > ../../concat/A2_5_ITS2_S188_run1_2_R1_001.fastq.gz\n",
        "cat A2_5a3_ITS2_S187_run1_2_R2_001.fastq.gz A2_5a4_ITS2_S188_run1_2_R2_001.fastq.gz > ../../concat/A2_5_ITS2_S188_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A2_6a3_ITS2_S189_run1_2_R1_001.fastq.gz A2_6a4_ITS2_S190_run1_2_R1_001.fastq.gz > ../../concat/A2_6_ITS2_S190_run1_2_R1_001.fastq.gz\n",
        "cat A2_6a3_ITS2_S189_run1_2_R2_001.fastq.gz A2_6a4_ITS2_S190_run1_2_R2_001.fastq.gz > ../../concat/A2_6_ITS2_S190_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat A2_7a3_ITS2_S193_run1_2_R1_001.fastq.gz A2_7a4_ITS2_S194_run1_2_R1_001.fastq.gz > ../../concat/A2_7_ITS2_S194_run1_2_R1_001.fastq.gz\n",
        "cat A2_7a3_ITS2_S193_run1_2_R2_001.fastq.gz A2_7a4_ITS2_S194_run1_2_R2_001.fastq.gz > ../../concat/A2_7_ITS2_S194_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S1_5a1_ITS2_S155_run1_2_R1_001.fastq.gz S1_5a2_ITS2_S156_run1_2_R1_001.fastq.gz > ../../concat/S1_5_ITS2_S156_run1_2_R1_001.fastq.gz\n",
        "cat S1_5a1_ITS2_S155_run1_2_R2_001.fastq.gz S1_5a2_ITS2_S156_run1_2_R2_001.fastq.gz > ../../concat/S1_5_ITS2_S156_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S1_6a1_ITS2_S157_run1_2_R1_001.fastq.gz S1_6a2_ITS2_S158_run1_2_R1_001.fastq.gz > ../../concat/S1_6_ITS2_S158_run1_2_R1_001.fastq.gz\n",
        "cat S1_6a1_ITS2_S157_run1_2_R2_001.fastq.gz S1_6a2_ITS2_S158_run1_2_R2_001.fastq.gz > ../../concat/S1_6_ITS2_S158_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S1_7a1_ITS2_S159_run1_2_R1_001.fastq.gz S1_7a2_ITS2_S160_run1_2_R1_001.fastq.gz > ../../concat/S1_7_ITS2_S160_run1_2_R1_001.fastq.gz\n",
        "cat S1_7a1_ITS2_S159_run1_2_R2_001.fastq.gz S1_7a2_ITS2_S160_run1_2_R2_001.fastq.gz > ../../concat/S1_7_ITS2_S160_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S1_8a3_ITS2_S161_run1_2_R1_001.fastq.gz S1_8a4_ITS2_S162_run1_2_R1_001.fastq.gz > ../../concat/S1_8_ITS2_S162_run1_2_R1_001.fastq.gz\n",
        "cat S1_8a3_ITS2_S161_run1_2_R2_001.fastq.gz S1_8a4_ITS2_S162_run1_2_R2_001.fastq.gz > ../../concat/S1_8_ITS2_S162_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S2_4a3_ITS2_S163_run1_2_R1_001.fastq.gz S2_4a4_ITS2_S164_run1_2_R1_001.fastq.gz > ../../concat/S2_4_ITS2_S164_run1_2_R1_001.fastq.gz\n",
        "cat S2_4a3_ITS2_S163_run1_2_R2_001.fastq.gz S2_4a4_ITS2_S164_run1_2_R2_001.fastq.gz > ../../concat/S2_4_ITS2_S164_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S2_5a1_ITS2_S165_run1_2_R1_001.fastq.gz S2_5a2_ITS2_S166_run1_2_R1_001.fastq.gz > ../../concat/S2_5_ITS2_S166_run1_2_R1_001.fastq.gz\n",
        "cat S2_5a1_ITS2_S165_run1_2_R2_001.fastq.gz S2_5a2_ITS2_S166_run1_2_R2_001.fastq.gz > ../../concat/S2_5_ITS2_S166_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S2_6a1_ITS2_S167_run1_2_R1_001.fastq.gz S2_6a2_ITS2_S168_run1_2_R1_001.fastq.gz > ../../concat/S2_6_ITS2_S168_run1_2_R1_001.fastq.gz\n",
        "cat S2_6a1_ITS2_S167_run1_2_R2_001.fastq.gz S2_6a2_ITS2_S168_run1_2_R2_001.fastq.gz > ../../concat/S2_6_ITS2_S168_run1_2_R2_001.fastq.gz\n",
        "\n",
        "cat S2_9a1_ITS2_S169_run1_2_R1_001.fastq.gz S2_9a2_ITS2_S170_run1_2_R1_001.fastq.gz > ../../concat/S2_9_ITS2_S170_run1_2_R1_001.fastq.gz\n",
        "cat S2_9a1_ITS2_S169_run1_2_R2_001.fastq.gz S2_9a2_ITS2_S170_run1_2_R2_001.fastq.gz > ../../concat/S2_9_ITS2_S170_run1_2_R2_001.fastq.gz\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "Clean up\n",
        "```\n",
        "> mv ../run1_2/* -t ../../concat/\n",
        "> rm concat-env1460184_out\n",
        "> mkdir ../../slurm_scripts\n",
        "> mv ./concat_env.sh ../../slurm_scripts\n",
        "> cd ../../\n",
        "> mv concat.sh slurm_scripts\n",
        "> rm concat-1455768_out\n",
        "> cd /flash/HusnikU/Vera/Symbiodiniaceae/concat/\n",
        "> mv F2-5_S72_L001_R2_001.fastq.gz F2_5_S72_L001_R2_001.fastq.gz\n",
        "> mv F2-6_S73_L001_R2_001.fastq.gz F2_6_S73_L001_R2_001.fastq.gz/flash/HusnikU/ > mv F2-5_S72_L001_R1_001.fastq.gz F2_5_S72_L001_R1_001.fastq.gz\n",
        "> mv F2-6_S73_L001_R1_001.fastq.gz F2_6_S73_L001_R1_001.fastq.gz\n",
        "\n",
        "```\n",
        "Now folder `concat` contains all the sequencing files concatenated between runs and different aliquots of the samples. The data  for sediment and algae concatenated between runs but not between the aliquots is in `raw_seq/env`. Directories `run1, run2, run3` in `raw/seq` contain raw sequencing files (some were renamed)."
      ],
      "metadata": {
        "id": "sSzbdMoDOXBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check reads statistics (seqkit)\n",
        "Slurm script `seqkit_stats.sh` (Run time 00:04:58)\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=seqkit_stats\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=10G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=10\n",
        "#SBATCH --output=seqkit_primer_start_check-%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module purge # make sure no modules are loaded\n",
        "module load bioinfo-ugrp-modules\n",
        "module load DebianMed/12.0\n",
        "module load seqkit/2.3.1+ds-1+b4\n",
        "\n",
        "path=\"/flash/HusnikU/Vera/Symbiodiniaceae/concat\"\n",
        "\n",
        "seqkit stats --basename --tabular ${path}/*fastq.gz> concat_stats.txt\n",
        "```\n",
        "\n",
        "Clean up\n",
        "```\n",
        "> mkdir stats\n",
        "> mv *stats.txt -t ./stats/\n",
        "> mv seqkit_stats.sh -t ./slurm_scripts/\n",
        "> rm *out\n",
        "```\n"
      ],
      "metadata": {
        "id": "mUmpsZojOXSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Symportal"
      ],
      "metadata": {
        "id": "EmqJ_DsufrvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SymPortal Installation"
      ],
      "metadata": {
        "id": "ATbHNJdhfxd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the SymPortal code was run with python version `Python 3.8.15`."
      ],
      "metadata": {
        "id": "4rmUUmxQGTDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SymPortal installation following the\n",
        "https://github.com/reefgenomics/SymPortal_framework/wiki/0_3_18_19_SymPortal-setup .\n",
        "\n",
        "In this case instead of using the `.yml` file, the separate installation of each package is used due to an error produces when using `.yml` file.\n",
        "\n",
        "```\n",
        "# download code from github\n",
        "# configure settings.py with the secret key\n",
        "# configure sp_config.py with user_name and user_email\n",
        "\n",
        "# Create a conda environment\n",
        "conda create --name symportal_env python=3.8\n",
        "\n",
        "# install packages\n",
        "# theoretically all the packages can be installed using .yml file\n",
        "conda install -y -c conda-forge plumbum matplotlib django biopython scipy==1.5.3 scikit-bio==0.5.6  werkzeug textdistance\n",
        "conda install -y -c bioconda blast=2.14.0 iqtree=2.2.2.3 mafft=7.520 mothur==1.43.0\n",
        "conda install pandas==1.5.3  # with later version, there is an error with appending to dataframe\n",
        "conda install conda-forge::xlrd\n",
        "conda install conda-forge::openpyxl\n",
        "conda install psycopg2\n",
        "conda install beautifulsoup4\n",
        "\n",
        "# navigate to SymPortal_framework directory\n",
        "conda activate symportal_env # activate env\n",
        "python3 manage.py migrate # creates a database\n",
        "python3 populate_db_ref_seqs.py # populates database\n",
        "python3 populate_db_unique_DIV_seqs.py # adds more named sequences to the database\n",
        "\n",
        "# run tests\n",
        "python3 -m tests.tests\n",
        "```\n",
        "\n",
        "Custom populate_db_ref_seqs.py files include changes in the following lines (rest is the same as the provided script):\n",
        "\n",
        "```\n",
        "# to add RefSeq sequences as instructed in GitHub\n",
        "fasta_path = os.path.abspath(os.path.join(os.path.dirname(__file__), 'symbiodiniaceaeDB', 'refSeqDB.fa'))\n",
        "```\n",
        "\n",
        "To add DIVs, need to download and unpack `SymPortal_unique_DIVs.fasta` from https://symportal.org/. It is a tar archive even though extention is `.tar.gz`\n",
        "```\n",
        "fasta_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './', 'SymPortal_unique_DIVs.fasta'))\n",
        "```\n",
        "\n",
        "<details>\n",
        "  <summary>SymPortal's post-MED seqs ('SymPortal_unique_post-MED.fasta') were not added  to the database directly with modified populate_db_ref_seqs.py [click to show details]</summary>\n",
        "\n",
        " It caused an error (\"key error\") later in the SymPortal analysis. This is caused by the fact that the names look like number-underscore-letter (738782_A), and one of the SymPortal scripts assumes that this number corresponds to the id of the sequence in the database though it does not. In case of 738782_A, the following error was noticed:\n",
        "\n",
        "```\n",
        " \"/home/v/vera-emelianenko/SymPortal_framework-master/output.py\", line 1493, in _add_uids_for_seqs_to_dfs\n",
        "    self.output_seqs_fasta_as_list.append(no_name_dict[col_name_id])\n",
        "KeyError: 738782\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ],
      "metadata": {
        "id": "fti4aE-htUjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SymPortal run consists of two steps: 1) data loading with `--load` option (generates DIVs - defining intragenomic variants) and data analysis with `--analyse` option (generates ITS2 profiles, taxonomic units of SymPortal). Detailed description at https://github.com/reefgenomics/SymPortal_framework/wiki/The-SymPortal-logic#a-brief-introduction-to-symportal.\n",
        "\n",
        "Here, Symportal data loading is first run on environmental samples to check if the aliquots taken grom the same sample are the same; then, DIVs are assigned for all the samples (data loading); then, data loading and ITS2 type profile assignment is performed only on host samples."
      ],
      "metadata": {
        "id": "xZt__QCPBpyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SymPortal run on sediment and algal samples only (DIVs)"
      ],
      "metadata": {
        "id": "dsgmdiSsgIe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the SymPortal run, metadata table (in excel) with all the file names is needed. The table has many columns, but for the program run, only the first three are important."
      ],
      "metadata": {
        "id": "pDoUU8XdWLey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get all the filenames for sediment and algae, list the files\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "> cd raw_seq/env/\n",
        "# use find instead of ls to print the full path to the files\n",
        "> find \"$PWD\" -type f | grep R1_001.fastq.gz | sort #list read1\n",
        "> find \"$PWD\" -type f | grep R2_001.fastq.gz | sort #list read2\n",
        "# manually copy file names to /flash/HusnikU/Vera/Symbiodiniaceae/meta_env.xlsx to two separate columns, for read1 and read2\n",
        "# manually assign sample names such as Sediment1-5a1, Algae2-5a3, etc.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uv_ZL_w8x8X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Run Symportal with the environmental data with different aliquots\n",
        "```\n",
        "conda activate symportal_env\n",
        "```\n",
        "\n",
        "Create slurm script symportal_env.sh\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=symportal_env\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=symportal_env_%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "#need to conda activate symportal_env before running\n",
        "#need to have data where the programs can write\n",
        "#in this case on flash, not bucket\n",
        "#need to copy (create) metadata file\n",
        "\n",
        "module purge\n",
        "\n",
        "~/SymPortal_framework-master/main.py --load /flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/env --name env --num_proc 40 --data_sheet /flash/HusnikU/Vera/Symbiodiniaceae/meta_env.xlsx\n",
        "```\n"
      ],
      "metadata": {
        "id": "ju9-jRqb3Rqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output files are in the same directory as SymPortal installation in the folder \"outputs\", then in the folder representing current run (in this case run6 as there were previous runs) and inside that - folder with timestamp. In this case it is\n",
        "`/home/v/vera-emelianenko/SymPortal_framework-master/outputs/loaded_data_sets/6/20250805T012727`. The output files are further analysed with Symbiodiniaceae_R_plots.Rmd. Since most of the aliquots do not differ from each other, data from different aliquots is concatenated and analysed as one sample."
      ],
      "metadata": {
        "id": "9Wv-qhtkf-XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up and file copying\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        ">rm mothur*\n",
        ">mv symportal_env* ./slurm_scripts/\n",
        "# from directory where R scripts will be run (laptop, not cluster) - copy files  \n",
        "# navigate to the directory in command line\n",
        "# copy recursively (-r) from oist:home... to current directory (./)\n",
        "> scp -r oist:/home/v/vera-emelianenko/SymPortal_framework-master/outputs/loaded_data_sets/6/20250805T012727 ./\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_CXWcFaC_xWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick sanity check of the results can be done by checking the plot `20250805T012727_seq_abundance_stacked_bar_plot.png` in `20250805T012727/post_med_seqs`\n",
        "\n",
        "There might be an error produced at the step where SymPortal tries to compare samples between each other, and script might be unable to finish the run. In this case, such an error was introduced when running SymPortal on the unmerged environmental data, but not when using merged environmental data with the rest of the samples. Despite this error, if the DIVs tables are already written, they can be used in the downstream analysis.\n",
        "\n",
        "<details>\n",
        "  <summary>Error text [click to expand]</summary>\n",
        "\n",
        "```\n",
        "/home/v/vera-emelianenko/SymPortal_framework-master/distance.py:1048: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
        "  renamed_pcoa_dataframe = renamed_pcoa_dataframe.append(\n",
        "calculating PCoA coordinates/home/v/vera-emelianenko/miniforge3/envs/symportal_env/lib/python3.8/site-packages/skbio/stats/ordination/_principal_coordinate_analysis.py:143: RuntimeWarning: The result contains negative eigenvalues. Please compare their magnitude with the magnitude of some of the largest positive eigenvalues. If the negative ones are smaller, it's probably safe to ignore them, but if they are large in magnitude, the results won't be useful. See the Notes section for more details. The smallest eigenvalue is -0.016884364325027722 and the largest is 1.6426709340416903.\n",
        "  warn(\n",
        "/home/v/vera-emelianenko/SymPortal_framework-master/distance.py:1048: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
        "  renamed_pcoa_dataframe = renamed_pcoa_dataframe.append(\n",
        "```\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "1yPF0Vr8v-y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SymPortal run on all the samples (DIVs)"
      ],
      "metadata": {
        "id": "Ny5KREbcf0qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as with sediment and algae only, first get all the filenames with their path:\n",
        "\n",
        "```\n",
        "> cd /flash/HusnikU/Vera/Symbiodiniaceae/concat\n",
        "# use find instead of ls to print the full path to the files\n",
        "> find \"$PWD\" -type f | grep R1_001.fastq.gz | sort #list read1\n",
        "> find \"$PWD\" -type f | grep R2_001.fastq.gz | sort #list read2\n",
        "# manually copy file names to /flash/HusnikU/Vera/Symbiodiniaceae/meta.xlsx to two separate columns, for read1 and read2\n",
        "# manually assign sample names such as Sediment1-5a1, Algae2-5a3, etc.\n",
        "```\n"
      ],
      "metadata": {
        "id": "NqtLjjsPgnrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and load slurm script `symportal_all.sh`\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=symportal_all\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=symportal_all_%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "#need to conda activate symportal_env before running\n",
        "#need to have data where the programs can write\n",
        "#in this case on flash, not bucket\n",
        "#need to copy (create) metadata file\n",
        "\n",
        "module purge\n",
        "\n",
        "~/SymPortal_framework-master/main.py --load /flash/HusnikU/Vera/Symbiodiniaceae/concat --name all --num_proc 40 --data_sheet /flash/HusnikU/Vera/Symbiodiniaceae/meta.xlsx\n",
        "```"
      ],
      "metadata": {
        "id": "Sa5Djux569RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The run has been completed successfully with run time 00:59:54\n",
        "```\n",
        "DATA LOADING COMPLETE\n",
        "DataSet id: 8\n",
        "DataSet name: all\n",
        "Loading completed in 3584.970986366272s\n",
        "DataSet loading_complete_time_stamp: 20250805T080909\n",
        "```"
      ],
      "metadata": {
        "id": "IO2lNLmsS7CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SymPortal run on host sample (ITS2 profiles)"
      ],
      "metadata": {
        "id": "6Hk5HDmFgC7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate ITS2 profles, only host data can be used: it does not make sence to generate ITS2 profiles for environmental samples as profiles assume that if multiple sequences from the same genus are present in the sample, they belong to the same taxonomic unit of symbiont, which does not hold true in case of environmental samples.\n",
        "See more about SymPortal's logic at the program's wiki: https://github.com/reefgenomics/SymPortal_framework/wiki/The-SymPortal-logic#a-brief-introduction-to-symportal"
      ],
      "metadata": {
        "id": "b1r7O4fqBxWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data load for host samples only"
      ],
      "metadata": {
        "id": "11TlZ1XGKcfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run SymPortal data loading module only with the host samples, using the same directory as for all of the samples but modified metadata table. The new table `meta_hosts.xlsx` is similar to `meta.xlsx` but the sample types Algae, Water, and Sediment were excluded (table edited in Excel manually).\n",
        "\n",
        "Create and run `symportal_hosts.sh`\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=symportal_hosts\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=symportal_hosts_%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "#need to conda activate symportal_env before running\n",
        "#need to have data where the programs can write\n",
        "#in this case on flash, not bucket\n",
        "#need to copy (create) metadata file meta_hosts.xlsx\n",
        "\n",
        "module purge\n",
        "\n",
        "~/SymPortal_framework-master/main.py --load /flash/HusnikU/Vera/concat --name hosts --num_proc 40 --data_sheet /flash/HusnikU/Vera/Symbiodiniaceae/meta_hosts.xlsx\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wQOADSzOCJWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run was finished successfully with run time 00:25:48\n",
        "```\n",
        "DATA LOADING COMPLETE\n",
        "DataSet id: 9\n",
        "DataSet name: hosts\n",
        "Loading completed in 1541.369069814682s\n",
        "DataSet loading_complete_time_stamp: 20250805T073659\n",
        "```\n",
        "\n",
        "Clean-up and data copying\n",
        "```\n",
        "> rm mothur*\n",
        "> mv *sh ./slurm_scripts\n",
        "> mv *_out ./slurm_scripts/\n",
        "# from the directory for R notebook\n",
        "> scp -r oist:/home/v/vera-emelianenko/SymPortal_framework-master/outputs/loaded_data_sets/8/20250805T070924 ./\n",
        "> scp -r oist:/home/v/vera-emelianenko/SymPortal_framework-master/outputs/loaded_data_sets/9/20250805T071117 ./\n",
        "```"
      ],
      "metadata": {
        "id": "XvczGH6Fcdfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ITS2 profile assigment for host samples only"
      ],
      "metadata": {
        "id": "BCvj1xbaKhji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is completed, next step is to run SymPortal analysis module with `--analyse`.\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=symportal_host_analysis\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=80G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=20\n",
        "#SBATCH --output=symportal_host_analysis_%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "#need to conda activate symportal_env before running\n",
        "#need to change the number of the dataset after --analyse\n",
        "\n",
        "module purge\n",
        "\n",
        "~/SymPortal_framework-master/main.py --analyse 9 --name host_analysis --num_proc 20\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "nw1UxRR2TA5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data analysis has been finished successfully with run time 00:11:21\n",
        "\n",
        "```\n",
        "ANALYSIS COMPLETE: DataAnalysis:\n",
        "        name: host_analysis\n",
        "        UID: 3\n",
        "\n",
        "DataSet analysis_complete_time_stamp: 20250805T091202\n",
        "```\n",
        "\n",
        "\n",
        "Clean-up and data copying\n",
        "```\n",
        "> mv *sh ./slurm_scripts\n",
        "> mv *_out ./slurm_scripts/\n",
        "> rm -r concat/tempData/\n",
        "# from the directory for R notebook\n",
        "> scp -r oist:/home/v/vera-emelianenko/SymPortal_framework-master/outputs/analyses/3/20250805T090154 ./\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bF4f2NOr0I9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps in data analysis of the SymPortal output are performed  with R scripts within RStudio. These scripts can be found in a separate file."
      ],
      "metadata": {
        "id": "us3XO6ZEMCcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run DADA2"
      ],
      "metadata": {
        "id": "GTu-QOx9glP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "v_vfQZ3lQQjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version info"
      ],
      "metadata": {
        "id": "snpJXtQxJlWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DADA2 pipeline was run according to the DADA2 ITS Pipeline Workflow (1.8) https://benjjneb.github.io/dada2/ITS_workflow.html\n",
        "\n",
        "R package versions:\n",
        "\n",
        "\n",
        "\n",
        "`R version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"`\n",
        "\n",
        "`\"dada2\" â€˜1.26.0â€™`\n",
        "\n",
        "`\"ShortRead\"  â€˜1.56.1â€™`\n",
        "\n",
        "`\"ggplot2\" â€˜3.3.6â€™`\n",
        "\n",
        "Other programs:\n",
        "\n",
        "`cutadapt/4.2-1`\n",
        "\n",
        "`seqkit/2.3.1+ds-1+b4`\n"
      ],
      "metadata": {
        "id": "ZFyBMz6kOqvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary analysis â€” including BCL conversion, demultiplexing, and adapter trimming â€” was performed using bcl2fastq v2.20 at OIST Sequencing Section, with AdapterRead1 = CTGTCTCTTATACACATCT, AdapterRead2 not specified, and default  adapter stringency (0.9). Here, only the steps following adapter romoval are described."
      ],
      "metadata": {
        "id": "adU5AtWiQV0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the data originates from different sequencing runs, as in this case, it is recommended to run DADA2 separately on each sequencing run and thn merge the results using the\n",
        "> different machines can have different empirical relationships between the assigned quality scores and the error rates, hence it is recommended to learn the error rates on a run-by-run basis. [Sourse: Benjamin Callahan on dada2 Github](https://github.com/benjjneb/dada2/issues/716).\n",
        "\n",
        "So we go back to the `raw_seq` folder which contains the sequencing data from separate runs separately: run1, run2 and run3."
      ],
      "metadata": {
        "id": "8VQ8tLNwjkUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter out Ns (dada2)"
      ],
      "metadata": {
        "id": "i_iCwFTAJsYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In thie reference dada2 workflow, cutadapt is run inside R to trim adaptors. However, in our case it is run separately in the command line. So the DADA2 reference script is split intwo two parts - before and after running cutadapt."
      ],
      "metadata": {
        "id": "DflXD9TAYCzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primers that were used to amplify the sequences:\n",
        "\n",
        "ITS-DINO 5â€™-TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG-GTGAATTGCAGAACTCCGTG-3â€™\n",
        "\n",
        "ITS2Rev2 5â€™-GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG-CCTCCGCTTACTTATATGCTT-3â€™\n",
        "\n",
        "The second part of the primer is specific, so `GTGAATTGCAGAACTCCGTG` is the forward primer and `CCTCCGCTTACTTATATGCTT` is the reverse primer. The first patrt is Illumina overhangs which was needed for the second amplification round with indices."
      ],
      "metadata": {
        "id": "kBo194yhk2Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These R scripts were first run interactively in RStudio on a small set of data (code not provided). More intermediate checks (reads quality, different variables) was done during this initial trouble-shooting run. After the trouble-shooting, scripts were scaled up to include all the data and run as slurm scripts on HPC cluster to process all of the samples.\n",
        "\n",
        "Script `predada2.sh` (Run time 00:04:04)\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=predada2\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=predada2-%j_out.txt\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "ml load R/4.2.1\n",
        "Rscript predada2_run1.r\n",
        "Rscript predada2_run2.r\n",
        "Rscript predada2_run3.r\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Dw-ZzcUPYNyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three scipts are only different by their \"path\" variable and their \"R.data\" name. The rest is the same.\n",
        "\n",
        "Script `predada2_run1.r`\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "\n",
        "path <- \"/flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run1\"\n",
        "\n",
        "list.files(path) # this is optional, to see if the path is ok. When run as .r script, it will print the output in predada2-%j_out.txt.\n",
        "\n",
        "# create matching lists of forward and reverse reads\n",
        "fnFs <- sort(list.files(path, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "fnRs <- sort(list.files(path, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "\n",
        "# identify primers\n",
        "FWD <- \"GTGAATTGCAGAACTCCGTG\"  \n",
        "REV <- \"CCTCCGCTTACTTATATGCTT\"  \n",
        "\n",
        "\n",
        "allOrients <- function(primer) {\n",
        "    # Create all orientations of the input sequence\n",
        "    require(Biostrings)\n",
        "    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors\n",
        "    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),\n",
        "        RevComp = Biostrings::reverseComplement(dna))\n",
        "    return(sapply(orients, toString))  # Convert back to character vector\n",
        "}\n",
        "FWD.orients <- allOrients(FWD)\n",
        "REV.orients <- allOrients(REV)\n",
        "FWD.orients\n",
        "\n",
        "# the output should be\n",
        "# Forward Complement Reverse RevComp\n",
        "# \"GTGAATTGCAGAACTCCGTG\" \"CACTTAACGTCTTGAGGCAC\" \"GTGCCTCAAGACGTTAAGTG\" \"CACGGAGTTCTGCAATTCAC\"\n",
        "\n",
        "#Prefilter the sequences to remove Ns\n",
        "\n",
        "#create file lists with the filtN subdirectory\n",
        "\n",
        "fnFs.filtN <- file.path(path, \"filtN\", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory\n",
        "\n",
        "fnRs.filtN <- file.path(path, \"filtN\", basename(fnRs))\n",
        "\n",
        "# Perform filtering to remove Ns (maximum Ns allowed is 0)\n",
        "filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)\n",
        "\n",
        "primerHits <- function(primer, fn) {\n",
        "    # Counts number of reads in which the primer is found\n",
        "    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n",
        "    return(sum(nhits > 0))\n",
        "}\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))\n",
        "\n",
        "# Filenames inside concat/filtN/ will be the same as original filenames, so it is important to keep the directories ordered\n",
        "# Next step is to run cutadapt on our N-filtered reads\n",
        "\n",
        "save.image(file = \"predada2_run1_workspace.RData\")\n",
        "# save workspace image to have access to all of the objects such as filename lists and primer seqs\n",
        "```\n",
        "\n",
        "\n",
        "Script `predada2_run2.r`\n",
        "\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "\n",
        "path <- \"/flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run2\"\n",
        "\n",
        "list.files(path) # this is optional, to see if the path is ok. When run as .r script, it will print the output in predada2-%j_out.txt.\n",
        "\n",
        "# create matching lists of forward and reverse reads\n",
        "fnFs <- sort(list.files(path, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "fnRs <- sort(list.files(path, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "\n",
        "# identify primers\n",
        "FWD <- \"GTGAATTGCAGAACTCCGTG\"  \n",
        "REV <- \"CCTCCGCTTACTTATATGCTT\"  \n",
        "\n",
        "\n",
        "allOrients <- function(primer) {\n",
        "    # Create all orientations of the input sequence\n",
        "    require(Biostrings)\n",
        "    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors\n",
        "    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),\n",
        "        RevComp = Biostrings::reverseComplement(dna))\n",
        "    return(sapply(orients, toString))  # Convert back to character vector\n",
        "}\n",
        "FWD.orients <- allOrients(FWD)\n",
        "REV.orients <- allOrients(REV)\n",
        "FWD.orients\n",
        "\n",
        "# the output should be\n",
        "# Forward Complement Reverse RevComp\n",
        "# \"GTGAATTGCAGAACTCCGTG\" \"CACTTAACGTCTTGAGGCAC\" \"GTGCCTCAAGACGTTAAGTG\" \"CACGGAGTTCTGCAATTCAC\"\n",
        "\n",
        "#Prefilter the sequences to remove Ns\n",
        "\n",
        "#create file lists with the filtN subdirectory\n",
        "\n",
        "fnFs.filtN <- file.path(path, \"filtN\", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory\n",
        "\n",
        "fnRs.filtN <- file.path(path, \"filtN\", basename(fnRs))\n",
        "\n",
        "# Perform filtering to remove Ns (maximum Ns allowed is 0)\n",
        "filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)\n",
        "\n",
        "primerHits <- function(primer, fn) {\n",
        "    # Counts number of reads in which the primer is found\n",
        "    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n",
        "    return(sum(nhits > 0))\n",
        "}\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))\n",
        "\n",
        "# Filenames inside concat/filtN/ will be the same as original filenames, so it is important to keep the directories ordered\n",
        "# Next step is to run cutadapt on our N-filtered reads\n",
        "\n",
        "save.image(file = \"predada2_run2_workspace.RData\")\n",
        "# save workspace image to have access to all of the objects such as filename lists and primer seqs\n",
        "```\n",
        "\n",
        "\n",
        "Script `predada2_run3.r`\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "\n",
        "path <- \"/flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run3\"\n",
        "\n",
        "list.files(path) # this is optional, to see if the path is ok. When run as .r script, it will print the output in predada2-%j_out.txt.\n",
        "\n",
        "# create matching lists of forward and reverse reads\n",
        "fnFs <- sort(list.files(path, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "fnRs <- sort(list.files(path, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "\n",
        "# identify primers\n",
        "FWD <- \"GTGAATTGCAGAACTCCGTG\"  \n",
        "REV <- \"CCTCCGCTTACTTATATGCTT\"  \n",
        "\n",
        "\n",
        "allOrients <- function(primer) {\n",
        "    # Create all orientations of the input sequence\n",
        "    require(Biostrings)\n",
        "    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors\n",
        "    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),\n",
        "        RevComp = Biostrings::reverseComplement(dna))\n",
        "    return(sapply(orients, toString))  # Convert back to character vector\n",
        "}\n",
        "FWD.orients <- allOrients(FWD)\n",
        "REV.orients <- allOrients(REV)\n",
        "FWD.orients\n",
        "\n",
        "# the output should be\n",
        "# Forward Complement Reverse RevComp\n",
        "# \"GTGAATTGCAGAACTCCGTG\" \"CACTTAACGTCTTGAGGCAC\" \"GTGCCTCAAGACGTTAAGTG\" \"CACGGAGTTCTGCAATTCAC\"\n",
        "\n",
        "#Prefilter the sequences to remove Ns\n",
        "\n",
        "#create file lists with the filtN subdirectory\n",
        "\n",
        "fnFs.filtN <- file.path(path, \"filtN\", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory\n",
        "\n",
        "fnRs.filtN <- file.path(path, \"filtN\", basename(fnRs))\n",
        "\n",
        "# Perform filtering to remove Ns (maximum Ns allowed is 0)\n",
        "filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)\n",
        "\n",
        "primerHits <- function(primer, fn) {\n",
        "    # Counts number of reads in which the primer is found\n",
        "    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n",
        "    return(sum(nhits > 0))\n",
        "}\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))\n",
        "\n",
        "# Filenames inside concat/filtN/ will be the same as original filenames, so it is important to keep the directories ordered\n",
        "# Next step is to run cutadapt on our N-filtered reads\n",
        "\n",
        "save.image(file = \"predada2_run3_workspace.RData\")\n",
        "# save workspace image to have access to all of the objects such as filename lists and primer seqs\n",
        "```"
      ],
      "metadata": {
        "id": "u2aLgp0qiCed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trim the primers (cutadapt)"
      ],
      "metadata": {
        "id": "VUwcfiSSKIPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "mkdir /flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run1/cutadapt\n",
        "mkdir /flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run2/cutadapt\n",
        "mkdir /flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run3/cutadapt\n",
        "```\n",
        "\n",
        "General usage of cutadapt for paired-end (from the program `-help`):\n",
        "```\n",
        "cutadapt -a ADAPT1 -A ADAPT2 [options] -o out1.fastq -p out2.fastq in1.fastq in2.fastq\n",
        "```\n",
        "Because of the variable length of the ITS2 region, we expect some amount  of reverse primers to be on the forward read in the reverse-complement orientation, and some amount of the forward primers to be on the reverse read in its reverse-complement orientation, so four primer sequences are supplied.\n",
        "\n",
        "Slurm script `cutadapt_r1.sh` (Run time 00:03:38):\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=cutadapt_r1\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=cutadapt-r1%j_out.txt\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module purge\n",
        "module load bioinfo-ugrp-modules\n",
        "module load DebianMed/12.0\n",
        "module load cutadapt/4.2-1\n",
        "\n",
        "path=\"/flash/HusnikU/Vera/Symbiodiniaceae/raw_seq/run1\"\n",
        "\n",
        "for file in ${path}/filtN/*R1_001.fastq.gz; do\n",
        "\n",
        "  sample=$(basename ${file%_R1_001.fastq.gz}) #for example  C1_3_ITS2_S108_run1_2 or F2_5_S72_L001\n",
        "\n",
        "  echo \"On sample: $sample\"\n",
        "\n",
        "  cutadapt \\\n",
        "  -g GTGAATTGCAGAACTCCGTG \\\n",
        "  -a AAGCATATAAGTAAGCGGAGG \\\n",
        "  -G CCTCCGCTTACTTATATGCTT \\\n",
        "  -A CACGGAGTTCTGCAATTCAC \\\n",
        "  -n 2 \\\n",
        "  -e 0.2 \\   \n",
        "  -j 40 \\\n",
        "  --discard-untrimmed \\\n",
        "  -o ${path}/cutadapt/${sample}_R1_001.fastq.gz \\\n",
        "  -p ${path}/cutadapt/${sample}_R2_001.fastq.gz \\\n",
        "  ${path}/filtN/${sample}_R1_001.fastq.gz \\\n",
        "  ${path}/filtN/${sample}_R2_001.fastq.gz \\\n",
        "  >> cutadapt_trimming_stats_run1.txt 2>&1  # write the output in the separate .txt file\n",
        "\n",
        "# -g sequence of an adapter ligated to the 5' end of the first read\n",
        "done\n",
        "# -a sequence of an adapter ligated to the 3' end of the first read\n",
        "# -G 5' adapter to be removed from R2\n",
        "# -A 3' adapter to be removed from R2\n",
        "# -n 2 Remove up to 2 adapters from each read\n",
        "# -e 0.2 maximum allowed error rate for full-length adapter match\n",
        "# -j number of CPU cores to use\n",
        "# --discard-untrimmed  discard reads that do not contain an adapter\n",
        "# -o FILE write trimmed reads to FILE\n",
        "# -p FILE write R2 to file\n",
        "# second to last line [FILE] read1 input\n",
        "# last line [FILE] read2 input\n",
        "\n",
        "echo \"Finished processing run1\"\n",
        "\n",
        "```\n",
        "\n",
        "Slurm scripts `cutadapt_r2.sh` (Run time 00:03:53) and `cutadapt_r3.sh` (run time 00:00:11) are the same but the directories are switched to run 2 and run3 instead of run1 and the output files are also changed to r3.\n",
        "\n",
        "\n",
        "Clean-up\n",
        "```\n",
        "> rm cutadapt*out.txt\n",
        "> mv *stats.txt stats\n",
        "> mv *sh slurm_scripts/\n",
        "> mv predada2*out.txt predada2.r r_scripts\n",
        "```"
      ],
      "metadata": {
        "id": "WdLkMqckWoBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign ASVs (dada2)"
      ],
      "metadata": {
        "id": "3ecjgyF8E-rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run dada2 to assign ASVs separately on each sequencing run.\n",
        "\n",
        "Slurm script `dada2-ASV.sh` (Run time 00:25:05):\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=dada2-ASV\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=dada2-ASV-%j_out.txt\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module load R/4.2.1\n",
        "Rscript dada2-ASV-r1.r\n",
        "Rscript dada2-ASV-r2.r\n",
        "Rscript dada2-ASV-r3.r\n",
        "```"
      ],
      "metadata": {
        "id": "gPUJxkeoHWJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R script `dada2-ASV-r1.r`\n",
        "\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "library(\"ggplot2\")\n",
        "packageVersion(\"ggplot2\")\n",
        "\n",
        "load(\"/flash/HusnikU/Vera/Symbiodiniaceae/predada2_run1_workspace.RData\")\n",
        "\n",
        "# path with files that have been filtered to not contain Ns and trimmed from primers\n",
        "path.cut <- file.path(path, \"cutadapt\")\n",
        "fnFs.cut <- file.path(path.cut, basename(fnFs))\n",
        "fnRs.cut <- file.path(path.cut, basename(fnRs))\n",
        "\n",
        "# create file lists for files with trimmed adaptors\n",
        "cutFs <- sort(list.files(path.cut, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "cutRs <- sort(list.files(path.cut, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "head (cutFs)\n",
        "\n",
        "# create file list for the files that will be quality-filtered\n",
        "filtFs <- file.path(path.cut, \"filtered\", basename(cutFs))\n",
        "filtRs <- file.path(path.cut, \"filtered\", basename(cutRs))\n",
        "head(filtFs)\n",
        "\n",
        "#Check primer presence in cutadapted reads (should be none)\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))\n",
        "\n",
        "# Extract sample names - first two parts separated by an underscore:\n",
        "get.sample.name <- function(fname) {\n",
        "  parts <- strsplit(basename(fname), \"_\")[[1]]\n",
        "  paste(parts[1:2], collapse = \"_\")\n",
        "}\n",
        "sample.names <- unname(sapply(cutFs, get.sample.name))\n",
        "head(sample.names)\n",
        "\n",
        "# Trim with filtering parameters: no Ns (maxN=0) (should already be satisfied), truncQ = 2 (truncate reads at the first instance of a quality score less than or equal to 2), rm.phix = TRUE (remove matches to phix genome, bacteriophage genome commonly used as sequencing control), maxEE=2,4 (expected error in forward and reverse reads; here we expect maximum 2 errors in forward and 4 errors in reverse reads), minLen 50 (minimal length of 50 basepairs).\n",
        "\n",
        "out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 4), truncQ = 2,\n",
        "    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE\n",
        "head(out)\n",
        "\n",
        "# check how many reads were filtered out\n",
        "filtered_out <- as.data.frame(out)\n",
        "filtered_out[,3] <- filtered_out$reads.out*100/filtered_out$reads.in\n",
        "write.table(filtered_out, 'filtered_out.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# learn error rates\n",
        "errF <- learnErrors(filtFs, multithread = TRUE)\n",
        "errR <- learnErrors(filtRs, multithread = TRUE)\n",
        "\n",
        "# sample inference\n",
        "dadaFs <- dada(filtFs, err = errF, multithread = TRUE)\n",
        "dadaRs <- dada(filtRs, err = errR, multithread = TRUE)\n",
        "\n",
        "# merge paired reads\n",
        "mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)\n",
        "# explore the dataset\n",
        "head(mergers[[1]])\n",
        "\n",
        "# construct sequence table (ASV table)\n",
        "seqtab <- makeSequenceTable(mergers)\n",
        "dim(seqtab)\n",
        "write.table(seqtab, 'seqtab_run1.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# inspect distribution of sequence lengths\n",
        "table(nchar(getSequences(seqtab)))\n",
        "pdf('Distribution_of_sequence_lengths_seqtab.pdf')\n",
        "hist(nchar(getSequences(seqtab)), main=\"Distribution of sequence lengths\")\n",
        "dev.off()\n",
        "\n",
        "# save the sequence table\n",
        "saveRDS(seqtab, \"run1.rds\")\n",
        "```\n",
        "\n",
        "\n",
        "R script `dada2-ASV-r2.r`\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "library(\"ggplot2\")\n",
        "packageVersion(\"ggplot2\")\n",
        "\n",
        "load(\"/flash/HusnikU/Vera/Symbiodiniaceae/predada2_run2_workspace.RData\")\n",
        "\n",
        "# path with files that have been filtered to not contain Ns and trimmed from primers\n",
        "path.cut <- file.path(path, \"cutadapt\")\n",
        "fnFs.cut <- file.path(path.cut, basename(fnFs))\n",
        "fnRs.cut <- file.path(path.cut, basename(fnRs))\n",
        "\n",
        "# create file lists for files with trimmed adaptors\n",
        "cutFs <- sort(list.files(path.cut, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "cutRs <- sort(list.files(path.cut, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "head (cutFs)\n",
        "\n",
        "# create file list for the files that will be quality-filtered\n",
        "filtFs <- file.path(path.cut, \"filtered\", basename(cutFs))\n",
        "filtRs <- file.path(path.cut, \"filtered\", basename(cutRs))\n",
        "head(filtFs)\n",
        "\n",
        "#Check primer presence in cutadapted reads (should be none)\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))\n",
        "\n",
        "# Extract sample names - first two parts separated by an underscore:\n",
        "get.sample.name <- function(fname) {\n",
        "  parts <- strsplit(basename(fname), \"_\")[[1]]\n",
        "  paste(parts[1:2], collapse = \"_\")\n",
        "}\n",
        "sample.names <- unname(sapply(cutFs, get.sample.name))\n",
        "head(sample.names)\n",
        "\n",
        "\n",
        "# Trim with filtering parameters: no Ns (maxN=0) (should already be satisfied), truncQ = 2 (truncate reads at the first instance of a quality score less than or equal to 2), rm.phix = TRUE (remove matches to phix genome, bacteriophage genome commonly used as sequencing control), maxEE=2,4 (expected error in forward and reverse reads; here we expect maximum 2 errors in forward and 4 errors in reverse reads), minLen 50 (minimal length of 50 basepairs).\n",
        "\n",
        "out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 4), truncQ = 2,\n",
        "    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE\n",
        "head(out)\n",
        "\n",
        "# check how many reads were filtered out\n",
        "filtered_out <- as.data.frame(out)\n",
        "filtered_out[,3] <- filtered_out$reads.out*100/filtered_out$reads.in\n",
        "write.table(filtered_out, 'filtered_out.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# learn error rates\n",
        "errF <- learnErrors(filtFs, multithread = TRUE)\n",
        "errR <- learnErrors(filtRs, multithread = TRUE)\n",
        "\n",
        "# sample inference\n",
        "dadaFs <- dada(filtFs, err = errF, multithread = TRUE)\n",
        "dadaRs <- dada(filtRs, err = errR, multithread = TRUE)\n",
        "\n",
        "# merge paired reads\n",
        "mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)\n",
        "# explore the dataset\n",
        "head(mergers[[1]])\n",
        "\n",
        "# construct sequence table (ASV table)\n",
        "seqtab <- makeSequenceTable(mergers)\n",
        "dim(seqtab)\n",
        "write.table(seqtab, 'seqtab_run2.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# inspect distribution of sequence lengths\n",
        "table(nchar(getSequences(seqtab)))\n",
        "pdf('Distribution_of_sequence_lengths_seqtab.pdf')\n",
        "hist(nchar(getSequences(seqtab)), main=\"Distribution of sequence lengths\")\n",
        "dev.off()\n",
        "\n",
        "# save the sequence table\n",
        "saveRDS(seqtab, \"run2.rds\")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "R script `dada2-ASV-r3.r`\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "library(\"ggplot2\")\n",
        "packageVersion(\"ggplot2\")\n",
        "\n",
        "load(\"/flash/HusnikU/Vera/Symbiodiniaceae/predada2_run3_workspace.RData\")\n",
        "\n",
        "# path with files that have been filtered to not contain Ns and trimmed from primers\n",
        "path.cut <- file.path(path, \"cutadapt\")\n",
        "fnFs.cut <- file.path(path.cut, basename(fnFs))\n",
        "fnRs.cut <- file.path(path.cut, basename(fnRs))\n",
        "\n",
        "# create file lists for files with trimmed adaptors\n",
        "cutFs <- sort(list.files(path.cut, pattern = \"_R1_001.fastq.gz\", full.names = TRUE))\n",
        "cutRs <- sort(list.files(path.cut, pattern = \"_R2_001.fastq.gz\", full.names = TRUE))\n",
        "head (cutFs)\n",
        "\n",
        "# create file list for the files that will be quality-filtered\n",
        "filtFs <- file.path(path.cut, \"filtered\", basename(cutFs))\n",
        "filtRs <- file.path(path.cut, \"filtered\", basename(cutRs))\n",
        "head(filtFs)\n",
        "\n",
        "#Check primer presence in cutadapted reads (should be none)\n",
        "rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,\n",
        "    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,\n",
        "    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))\n",
        "\n",
        "# Extract sample names - first two parts separated by an underscore:\n",
        "get.sample.name <- function(fname) {\n",
        "  parts <- strsplit(basename(fname), \"_\")[[1]]\n",
        "  paste(parts[1:2], collapse = \"_\")\n",
        "}\n",
        "sample.names <- unname(sapply(cutFs, get.sample.name))\n",
        "head(sample.names)\n",
        "\n",
        "\n",
        "# Trim with filtering parameters: no Ns (maxN=0) (should already be satisfied), truncQ = 2 (truncate reads at the first instance of a quality score less than or equal to 2), rm.phix = TRUE (remove matches to phix genome, bacteriophage genome commonly used as sequencing control), maxEE=2,4 (expected error in forward and reverse reads; here we expect maximum 2 errors in forward and 4 errors in reverse reads), minLen 50 (minimal length of 50 basepairs).\n",
        "\n",
        "out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 4), truncQ = 2,\n",
        "    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE\n",
        "head(out)\n",
        "\n",
        "# check how many reads were filtered out\n",
        "filtered_out <- as.data.frame(out)\n",
        "filtered_out[,3] <- filtered_out$reads.out*100/filtered_out$reads.in\n",
        "write.table(filtered_out, 'filtered_out.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# learn error rates\n",
        "errF <- learnErrors(filtFs, multithread = TRUE)\n",
        "errR <- learnErrors(filtRs, multithread = TRUE)\n",
        "\n",
        "# sample inference\n",
        "dadaFs <- dada(filtFs, err = errF, multithread = TRUE)\n",
        "dadaRs <- dada(filtRs, err = errR, multithread = TRUE)\n",
        "\n",
        "# merge paired reads\n",
        "mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)\n",
        "# explore the dataset\n",
        "head(mergers[[1]])\n",
        "\n",
        "# construct sequence table (ASV table)\n",
        "seqtab <- makeSequenceTable(mergers)\n",
        "dim(seqtab)\n",
        "write.table(seqtab, 'seqtab_run3.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# inspect distribution of sequence lengths\n",
        "table(nchar(getSequences(seqtab)))\n",
        "pdf('Distribution_of_sequence_lengths_seqtab.pdf')\n",
        "hist(nchar(getSequences(seqtab)), main=\"Distribution of sequence lengths\")\n",
        "dev.off()\n",
        "\n",
        "# save the sequence table\n",
        "saveRDS(seqtab, \"run3.rds\")\n",
        "```"
      ],
      "metadata": {
        "id": "V5gJ72-mHbul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the tables and write final outputs\n",
        "\n",
        "Slurm script `dada2-merge.sh`\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=dada2-merge\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=100G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=40\n",
        "#SBATCH --output=dada2-merge-%j_out.txt\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module load R/4.2.1\n",
        "Rscript dada2-merge.r\n",
        "```\n",
        "Script `dada2-merge.r`\n",
        "```\n",
        "library(\"dada2\")\n",
        "packageVersion(\"dada2\")\n",
        "library(\"ShortRead\")\n",
        "packageVersion(\"ShortRead\")\n",
        "library(Biostrings)\n",
        "packageVersion(\"Biostrings\")\n",
        "library(\"ggplot2\")\n",
        "packageVersion(\"ggplot2\")\n",
        "\n",
        "### Join each run into a single sequence table\n",
        "run1 <- readRDS(\"run1.rds\")\n",
        "run2 <- readRDS(\"run2.rds\")\n",
        "run3 <- readRDS(\"run3.rds\")\n",
        "complete_seqtab <- mergeSequenceTables(run1, run2, run3)\n",
        "saveRDS(complete_seqtab, \"complete_seqtab_run1-2-3.rds\")\n",
        "\n",
        "seqtab.nochim <- removeBimeraDenovo(complete_seqtab, method = \"consensus\", multithread=TRUE, verbose=TRUE)\n",
        "saveRDS(seqtab.nochim, \"seqtab.nochim_run1-2-3.rds\")\n",
        "\n",
        "print(\"Calculating sum(seqtab.nochim)/sum(complete_seqtab)\")\n",
        "sum(seqtab.nochim)/sum(complete_seqtab)\n",
        "\n",
        "write.table(seqtab.nochim , 'seqtab_nochim_run1-2-3.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "# explore distribution of sequence length after chimera removal\n",
        "table(nchar(getSequences(seqtab.nochim)))\n",
        "pdf('Distribution_of_sequence_lengths_seqtab_nochim_run1-2-3.pdf')\n",
        "hist(nchar(getSequences(seqtab.nochim)), main=\"Distribution of sequence lengths no chimeras\")\n",
        "dev.off()\n",
        "\n",
        "# giving our seq headers more manageable names (ASV_1, ASV_2...)\n",
        "asv_seqs <- colnames(seqtab.nochim)\n",
        "asv_headers <- vector(dim(seqtab.nochim)[2], mode=\"character\")\n",
        "\n",
        "for (i in 1:dim(seqtab.nochim)[2]) {\n",
        "  asv_headers[i] <- paste(\">ASV\", i, sep=\"_\")\n",
        "}\n",
        "\n",
        "# making and writing out a fasta of our final ASV seqs:\n",
        "asv_fasta <- c(rbind(asv_headers, asv_seqs))\n",
        "\n",
        "write(asv_fasta, 'ASVs_run1-2-3.fasta')\n",
        "\n",
        "# count table:\n",
        "asv_tab <- t(seqtab.nochim)\n",
        "row.names(asv_tab) <- sub(\">\", \"\", asv_headers)\n",
        "write.table(asv_tab, 'ASVs_counts_run1-2-3.txt', sep=\"\\t\", quote=F)\n",
        "\n",
        "save.image(file = \"dada2-ASV_workspace.RData\")\n",
        "```\n",
        "```\n",
        "\"Calculating sum(seqtab.nochim)/sum(complete_seqtab)\"\n",
        "[1] 0.9939722\n",
        "\n",
        "table(nchar(getSequences(seqtab.nochim)))\n",
        " 62  64  77  78  82  85 101 156 164 165 166 167 169 176 177 178 179 183 184 185\n",
        "  1   1   1   1   2   1   1   1   1   2   3   1   2   1   7   1   1   1   1   1\n",
        "188 189 191 192 193 195 199 201 202 207 209 210 211 212 213 216 220 223 224 226\n",
        "  1   1   2   1   1   1   3   1   1   1   4   1   2   2   1   1   3   1   2   1\n",
        "227 228 231 232 233 234 235 237 238 239 241 242 243 244 246 247 248 249 250 251\n",
        "  1   3   1   3   3   3   3   3   1   2   1   3   3   1   2   4   4   4   5   3\n",
        "252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
        "  3   7   4   4   5   1   4   4   8   4   4   9   9  17   8   4  30  26  18  19\n",
        "272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
        " 15  27  15   8  14   3   1  14   9  13 116  11  17  19   9  14  23   8  14  24\n",
        "292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
        "  1  29   3  10  18   8  51  17 173  12  15  12  19  16  14  35  18  27  39  16\n",
        "312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331\n",
        " 19  13  22  24  12  11  16  13   7  13  12  20  16   5   4  12   5   5   1   3\n",
        "332 333 334 335 337 338 340 341 342 343 344 345 346 347 348 349 350 352 354 357\n",
        "  2   2   5   3   2   1   1   2   2   1   1   1   2   1   2   1   1   1   1   1\n",
        "365 367 375 376 379 380 384 386 387 388 392 393 394 395 401 402 403 415 426 439\n",
        "  1   2   1   2   2   1   3   7   2   1   1   4   8   2   2   1   1   2   1   1\n",
        "440 441 442 452 457 466 470 471 472 473 489 491 494 514 515 538\n",
        "  3   2   1   1   1   1   1   1   1   1   1   2   1   1   1   1\n",
        "```"
      ],
      "metadata": {
        "id": "TGsAFY4eIvgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up\n",
        "```\n",
        "> mv *.r r_scripts\n",
        "> mv *RData r_scripts\n",
        "> mkdir dada2_output\n",
        "> mv *pdf dada2_output\n",
        "> mv *sh -t slurm_scripts\n",
        "> mv *out.txt slurm_scripts\n",
        "> mv *rds *txt *fasta dada2_output\n",
        "# copy files to RStudio running directory (laptop)\n",
        "```"
      ],
      "metadata": {
        "id": "BWFXPCdlln0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Because the names of \"filtered_out\" were the same, the files from last two runs got overwritten.\n",
        "\n",
        "Get stats for the filtered samples\n",
        "`seqkit_stat_filtered.sh`\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=compute\n",
        "#SBATCH --job-name=seqkit_stat_filtered\n",
        "#SBATCH --time=2-0\n",
        "#SBATCH --mem=10G\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=2\n",
        "#SBATCH --output=seqkit_stat_filtered-%j_out\n",
        "#SBATCH --mail-type=FAIL,END\n",
        "#SBATCH --mail-user=vera.emelianenko@oist.jp\n",
        "\n",
        "module purge # make sure no modules are loaded\n",
        "module load bioinfo-ugrp-modules\n",
        "module load DebianMed/12.0\n",
        "module load seqkit/2.3.1+ds-1+b4\n",
        "\n",
        "path=\"raw_seq/run3/cutadapt/filtered/\"\n",
        "seqkit stats --basename --tabular ${path}/*fastq.gz> run3_filtered_stats.txt\n",
        "\n",
        "path=\"raw_seq/run2/cutadapt/filtered/\"\n",
        "seqkit stats --basename --tabular ${path}/*fastq.gz> run2_filtered_stats.txt\n",
        "\n",
        "path=\"raw_seq/run1/cutadapt/filtered/\"\n",
        "seqkit stats --basename --tabular ${path}/*fastq.gz> run1_filtered_stats.txt\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ro5Nk7qGhmab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With lower value of maxEE = (2,2) in one of the trial runs, most of the reads were filtered out due to the low quality, leaving ~10% only (higher values for the host samples, lower values for the environmental samples):\n",
        "\n",
        "```\n",
        "input   filtered        denoisedF       denoisedR       merged  nonchim\n",
        "A1_1    231300  29116   28732   28681   20412   20306\n",
        "A1_2    239369  60002   59651   59591   51934   51908\n",
        "A1_5    252076  49432   48914   48927   40016   39676\n",
        "A1_6    210436  31174   30781   30703   26913   26882\n",
        "A2_2    241582  66469   66407   66301   59309   58198\n",
        "A2_5    248745  40692   40378   40253   33195   32758\n",
        "A2_6    238337  42687   41993   42192   33008   33008\n",
        "A2_7    248917  42845   42406   42326   35307   35214\n",
        "An1     112359  29756   29743   29729   26134   25863\n",
        "An2     113436  35833   35784   35831   31887   31585\n",
        "C1_1    262907  83064   82989   83002   75000   74192\n",
        "C1_12   121814  43978   43914   43966   41748   41147\n",
        "C1_14   121887  24588   24556   24579   22729   22568\n",
        "C1_2    133278  28198   28164   28147   22561   22561\n",
        "C1_3    136650  37446   37446   37445   33626   33163\n",
        "C1_5    143676  33871   33803   33859   31511   31240\n",
        "C1_6    116395  43097   42756   43014   39929   39923\n",
        "C1_9    94652   17108   17100   17101   16097   15909\n",
        "C2_1    124291  36323   36301   36291   32421   32173\n",
        "...\n",
        "```\n",
        "There are different references in the literature.\n",
        "In ITS2 tutorial of DADA2, the standard (2,2) is used. In [Quigley et al., 2019](https://www.nature.com/articles/s41598-019-50045-y)  a stricter value of 1 is used (\"Reads were then filtered further, removing reads exhibiting matches to the phiX genome, reads with uncharacterized bases or reads with more than **one expected error**\" - from Supplementary material). On the other hand, in [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/full/10.1111/mec.15719) during investigation of free-living Symbiodiniaceae, a more relaxed parameter of  maxEE = c(2, 4) is used (\"**relaxed the EE to 4** for the reverse reads which typically are of lower quality\" - [GitHub reference](https://github.com/nitschkematthew/Free_living_Symbiodiniaceae_HI/blob/master/R/dada2_Heron/dada2_heron.Rmd)).\n",
        "We followed [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/full/10.1111/mec.15719) and used maxEE=c(2,2) accounting for the reverse reads that have low quality. Since we will use lulu later in the pipeline merging the co-occuring ASVs with > 95% identity, the erroneous ASVs with 4 mistakes over 300 bp will be nerged together."
      ],
      "metadata": {
        "id": "IE35kpbZjV0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge and filter the counts"
      ],
      "metadata": {
        "id": "euUUFyMN4mew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is in the file `lulu/Symbiodiniaceae_LULU.Rmd`."
      ],
      "metadata": {
        "id": "NmBdffJyobJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster similar co-occuring ASVs with LULU"
      ],
      "metadata": {
        "id": "KnGEyiEbUZPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used LULU R package to reduce the number of ASVs, following the workflow suggested in ([LULU GitHub](https://github.com/tobiasgf/lulu)).\n",
        "\n",
        "Code is in the file `lulu/Symbiodiniaceae_LULU.Rmd`."
      ],
      "metadata": {
        "id": "zINebmqQU3jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output files after running LULU:\n",
        "\n",
        "`ASVs_counts_curated.txt` - new curated ASV counts table\n",
        "\n",
        "`ASVs_curated.fasta` - new curated ASV sequences (subset of original ASVs.fasta)"
      ],
      "metadata": {
        "id": "ANd6bgN6NFjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We taxonomically classified all the sequences in DADA2 using the in-built assingTaxonomy() function."
      ],
      "metadata": {
        "id": "7-t7vOzxSeT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare database"
      ],
      "metadata": {
        "id": "b_Vq-q84wn-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select the database"
      ],
      "metadata": {
        "id": "W-KTd0zmZ9pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719), a database including ITS2 from other dinoflaggelates and organisms was used. However, this database was based on GenBank entries, which sometimes are not well curated especially with such complicated clade as Symbiodiniaceae. This database also did not contain clade J sequences which were not publiched at that moment.\n",
        "\n",
        "In [Olivares-Cordero et al., 2025](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1002/ece3.70839), a hybrid approach is presented where first DIVs are assigned by SymPortal and then used for DADA2 taxonomy assignment, essentially allowing to establish direct correspondence between ASV (often multiple) and DIV.\n",
        "\n",
        "Apart from SymPortal, another database used with dada2 for Symbiodiniaceae taxonomic assignment is GeoSymbio ([Franklin at al., 2011](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/full/10.1111/j.1755-0998.2011.03081.x)). However, this database is already integrated in the \"Symbiodatabaceae\" in [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719) (source: script [Symbiodatabaceae.Rmd](https://github.com/nitschkematthew/Symbiodatabaceae/blob/master/Symbiodatabaceae/Symbiodatabaceae.Rmd) at the [GitHub page Symbiodatabaceae](https://github.com/nitschkematthew/Symbiodatabaceae) supporting Fujice et al. paper). In addition, at the time of running present analysis (August 2025) the original GeoSymbio database could not be accessed: https://sites.google.com/site/geosymbio/ returned `Error 404. The requested URL was not found on this server.`\n",
        "\n",
        "Using only SymPortal database in our case was not preferable because the reason for using DADA2 in the first place was the inability of SymPortal to deal with clade J and Miliolidium sequences even if they were added to the SymPortal database. We decided to use the database from [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719), combined with SymPortal output fasta file and reference sequences of *Miliolidium* (were already present but we added more) and clade J."
      ],
      "metadata": {
        "id": "I6TOiTcewtia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get all unique sequences from SymPortal"
      ],
      "metadata": {
        "id": "woO33_38aDwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the most comprehensive SymPortal db out of all the named sequences and the sequences found in our set of samples\n",
        "\n",
        "Use `20250805T070924.seqs.fasta` (all DIV  sequences from the SymPortal run with all the samples, cab be found at Figshare 10.6084/m9.figshare.29881454) and two Symportal reference files (downloaded from the Symportal website and github) - `SymPortal_unique_DIVs.fasta` and `refSeqDB.fa`.\n",
        "\n",
        "```\n",
        "> cat 20250805T070924.seqs.fasta  SymPortal_unique_DIVs.fasta  refSeqDB.fa | seqkit rmdup -o SymPortal_DIVs_all.fasta\n",
        "# check if there are sequence duplicates\n",
        "> cat SymPortal_DIVs_all.fasta | seqkit rmdup -s -d duplicated.fasta -D duplicated.detail.txt -o test.fasta\n",
        "```\n",
        "There are some sequrnces with the same sequence but different name:\n",
        "```\n",
        "3\tA5d/A5, A5d, A5\n",
        "3\tC57/C57d, C57, C57d\n",
        "3\tC15L/C15bq, C15L, C15bq\n",
        "3\tA6, A6/A6b, A6b\n",
        "3\tC1bo, C1.6/C1bo, C1.6\n",
        "3\tC15bb, C15.9/C15bb, C15.9\n",
        "3\tC12/C12c, C12, C12c\n",
        "3\tC21ac, C21.11/C21ac, C21.11\n",
        "3\tC17.2/C17d, C17.2, C17d\n",
        "2\tC15mo/C15.1, C15.1\n",
        "2\tC40b/C40z, C40b\n",
        "2\tC3.14/C3te, C3.14\n",
        "2\tC1.7/C1lk, C1.7\n",
        "2\tA1.2/A1kv, A1.2\n",
        "2\tF5.2d, F5.2d/F5ae\n",
        "2\tF5.1a/F5u, F5.1a\n",
        "2\tA3b/A3bi, A3b\n",
        "2\tC45f/C45.3, C45.3\n",
        "2\tC67/C67b, C67\n",
        "2\tC21cf/C21.12, C21.12\n",
        "```\n",
        "\n",
        "Check which ones are duplicated in the SymPortal run output:\n",
        "\n",
        "```\n",
        "> seqkit grep -f dublicated.txt 20250805T070924.seqs.fasta -o duplicated_in_all.fasta\n",
        "```\n",
        "Sequences that are present in Symportal results:\n",
        "\n",
        "```\n",
        "A6\n",
        "A1.2/A1kv\n",
        "C21ac\n",
        "C15bb\n",
        "C3.14/C3te\n",
        "C1bo\n",
        "F5.1a/F5u\n",
        "```\n",
        "\n",
        "Remove duplicated sequences leaving the ones with more comprehensive names unless they were labeled differently in the output (file was edited manually to leave only the ids of sequences that need to be removed)\n",
        "```\n",
        "# grep all but remove.fasta which is a txt list of ids, not fasta\n",
        "> seqkit grep -f ./remove.fasta SymPortal_DIVs_all.fasta -o SymPortal_DIVs_all_clean.fasta -v  # -v means invert match\n",
        "```\n",
        "\n",
        "Clean up\n",
        "```\n",
        "> rm duplicated* remove.fasta test_clean.fasta dublicated.txt SymPortal_DIVs_all.fasta test.fasta\n",
        "> mkdir db_used\n",
        "> mv 20250805T070924.seqs.fasta SymPortal_unique_DIVs.fasta refSeqDB.fa ./db_used/\n",
        "```\n",
        "\n",
        "Files used\n",
        "\n",
        "<details>\n",
        "  <summary> dublicated.txt [click to show details]</summary>\n",
        "\n",
        "```\n",
        "A5d/A5\n",
        "A5d\n",
        "A5\n",
        "C57/C57d\n",
        "C57\n",
        "C57d\n",
        "C15L/C15bq\n",
        "C15L\n",
        "C15bq\n",
        "A6\n",
        "A6/A6b\n",
        "A6b\n",
        "C1bo\n",
        "C1.6/C1bo\n",
        "C1.6\n",
        "C15bb\n",
        "C15.9/C15bb\n",
        "C15.9\n",
        "C12/C12c\n",
        "C12\n",
        "C12c\n",
        "C21ac\n",
        "C21.11/C21ac\n",
        "C21.11\n",
        "C17.2/C17d\n",
        "C17.2\n",
        "C17d\n",
        "C15mo/C15.1\n",
        "C15.1\n",
        "C40b/C40z\n",
        "C40b\n",
        "C3.14/C3te\n",
        "C3.14\n",
        "C1.7/C1lk\n",
        "C1.7\n",
        "A1.2/A1kv\n",
        "A1.2\n",
        "F5.2dF5.2d/F5ae\n",
        "F5.1a/F5u\n",
        "F5.1a\n",
        "A3b/A3bi\n",
        "A3b\n",
        "C45f/C45.3\n",
        "C45.3\n",
        "C67/C67b\n",
        "C67\n",
        "C21cf/C21.12\n",
        "C21.12\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> remove.fasta [click to show details]</summary>\n",
        "\n",
        "```\n",
        "A5d\n",
        "A5\n",
        "C57\n",
        "C57d\n",
        "C15L\n",
        "C15bq\n",
        "A6/A6b\n",
        "A6b\n",
        "C1.6/C1bo\n",
        "C1.6\n",
        "C15.9/C15bb\n",
        "C15.9\n",
        "C12\n",
        "C12c\n",
        "C21.11/C21ac\n",
        "C21.11\n",
        "C17.2\n",
        "C17d\n",
        "C15.1\n",
        "C40b\n",
        "C3.14\n",
        "C1.7\n",
        "A1.2\n",
        "F5.2d\n",
        "F5.1a\n",
        "A3b\n",
        "C45.3\n",
        "C67\n",
        "C21.12\n",
        "```\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "B9nUgCBeD2Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SymPortal_DIVs_all_clean.fasta contains all the SymPortal named DIVs (such as A8b, F5.1a, I4c) and the DIVs that were found during the local analysis and don't have 100% match to the reference (5595_A, 5621_F, 5748_I)."
      ],
      "metadata": {
        "id": "LzBTPLbwRmQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and check the Symbiodatabaceae ITS2 database"
      ],
      "metadata": {
        "id": "4Beov73GaTHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the database from [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719).\n",
        "\n"
      ],
      "metadata": {
        "id": "k7HYlEZ5SYqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code and the database from this refeence paper were accessed at https://github.com/nitschkematthew/Symbiodatabaceae.\n",
        "\n",
        "The combined database `ITS2dbn1_Dinophy_Symbio.fasta` was downloaded from https://github.com/nitschkematthew/Symbiodatabaceae/blob/master/Combining_databases/ITS2dbn1_Dinophy_Symbio.fasta.\n",
        "It includes general ITS2 sequences (of eukaryotes), dinoflagellate sequences, and Symbiodiniaceae sequences. Format:\n",
        "\n",
        "```\n",
        ">Eukaryota;Streptophyta;unknown;Apiales;Apiaceae;'Oreofraga';'Oreofraga_morrisiana'\n",
        "CACATCGCGTCGCCCCACCGACACAGTCCGGGTGGGGCGGACAATGGCCTCCCGTGCATTCGACGCGTGGCTGGCCGAAA\n",
        "AGAAAGCACCCGGCGACGGACGCTGTGACATTGGTGGTTGTATCTATATACCTTCTCGTGTCGCTCGGTTACGCGTCACC\n",
        "GGTGCTAGATTGACCCTGCGGCGCCACCCACGGTGCGCGCYCCGA\n",
        ">Eukaryota;Streptophyta;unknown;Apiales;Apiaceae;'Rughidia';'Rughidia_milleri'\n",
        "ACGCATCGTCTTGCCCACAACCACTCACTCCTCGAGGAGCTGTGCTGGTTTGGGGGCGAAAATTGGCCTCCCGTGCTTTA\n",
        "TCATGCGGTTGGTGCAAAAGTGAGTCTTCGGTGACGGACGCCTTGACATTGGTGGTTGTAAAAGGCCCTCTTGTCTTGTC\n",
        "GGGCGAATCCATATCATCTTAGCGAGCTCCAAGACCCTTGGGCGGCATAAACTCTTTGCCCCTCGA\n",
        "```\n",
        "\n",
        "For Symbiodiniaceae, no species names are provided; instead, clade and accession numbers are given. If multiple accession numbers match the sequence, they are separated by \"|\".\n",
        "```\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Effrenium;E_AY160123\n",
        "AACCAATAGCACCCTGAACTCGCATTGCACTCTTGGGACACGCCTAAGAGTATGTCTGCTTCAGTGCTTTTTCATATCTT\n",
        "CGCAGCGCGGGCTTCCTGGAGAAGCCTTGAGCCTCTTTGCGCGCTGCTGCATCAGAATTTGCAGCGGCGCGCTGAACACA\n",
        "AACCGGGAGGTAAGCTGGACTGATTTGTCGCGCATCACTGGGCACGTGTGTCCGTTTTGGCCCAATCATGCCAGCCTGCC\n",
        "AAGCAATTGGTGCTCAAATACCAATCTTAGCATGAAGTCAGACAAGCAA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Effrenium;E_EU074907|E_EU074917|E_EU074918|E_EU074923\n",
        "AACCAATAGCACCCTGAACTCGCATTGCACTCTTGGGACACGCCTGAGAGTATGTCTGCTTCAGTGCTTTTCATATCTTC\n",
        "GCAGTGCGGGCTTCCTGGAGAAGCCTTGAGCCTCTTTGCGCGCTGCTGCATCAGAATTTGCAGCGGCGCGCTGAACACAA\n",
        "ACCGGGAGGTAAGCTGGACTGATTTGTCGCGCATCACTGGGCACGTGTGTCCGTTTTGGCCCAATCATGCCAGCCTGCCA\n",
        "AGCAATTGGTGCTCAAATACCAATCTTAGCATGAAGTCAGACAAGCAA\n",
        "```"
      ],
      "metadata": {
        "id": "sL_P2j9wTc9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the database.\n",
        "Since SymPortal and the downloaded database might use different Symbiodiniaceae classification, understanding what is the genus classification in the database is important for using it.\n",
        "\n",
        "What are different genus names used for Symbiodiniaceae?\n",
        "\n",
        "```\n",
        "> grep \"^>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae\" ITS2dbn1_Dinophy_Symbio.fasta | cut -d\";\" -f 6 | sort | uniq\n",
        "> grep \"^>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae\" ITS2dbn1_Dinophy_Symbio.fasta | cut -d\";\" -f 6,7 | sort | uniq > Symbiodiniaceae_names.txt\n",
        "```\n",
        "\n",
        "```\n",
        "Breviolum\n",
        "Cladocopium\n",
        "Durusdinium\n",
        "Effrenium\n",
        "Foraminifera_D\n",
        "Freudenthalidium\n",
        "Fugacium\n",
        "Gerakladium\n",
        "Halluxium\n",
        "Symbiodinium\n",
        "Symbiodinium_Fr2\n",
        "Symbiodinium_Fr4\n",
        "Symbiodinium_I\n",
        "```\n",
        "\n",
        "Change (for easiness of comprehention and personal preference):\n",
        "```\n",
        "sed -i 's/Cladocopium;Symbiodinium_GQ479800/Cladocopium;C_GQ479800/g' ITS2dbn1_Dinophy_Symbio.fasta # rename this \"species\" of Cladocopium as C_\n",
        "sed -i 's/Gerakladium;Symbiodinium_AB25378/Gerakladium;G_AB253788/g' ITS2dbn1_Dinophy_Symbio.fasta # rename this \"species\" of Gerakladium as as G_\n",
        "sed -i 's/Symbiodinium_/clade_/g' ITS2dbn1_Dinophy_Symbio.fasta # rename Symbiodinium_ to clade_ (Symbiodinium_I to clade_I)\n",
        "sed -i 's/ (type 1)/type_1/g' ITS2dbn1_Dinophy_Symbio.fasta # remove spaces and parenthesis  \n",
        "sed -i 's/ (type 2)/type_2/g'  ITS2dbn1_Dinophy_Symbio.fasta # remove spaces and parenthesis  \n",
        "sed -i 's/ _/_/g' ITS2dbn1_Dinophy_Symbio.fasta # remove spaces\n",
        "```"
      ],
      "metadata": {
        "id": "onL1IBc3aWNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Miliolidium sequences"
      ],
      "metadata": {
        "id": "512cLWWxYEkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Miliolidium* is described in: [Pochon, X. and LaJeunesse, T.C., 2021. *Miliolidium* n. gen, a new symbiodiniacean genus whose members associate with soritid foraminifera or are freeâ€living. Journal of Eukaryotic Microbiology, 68(4), p.e12856](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/full/10.1111/jeu.12856)\n",
        "\n",
        "From Figure2 B, there are at least 5 distinct ITS2 variants in this genus: *M. leei* (rt401), M. sp. from Marginopora in Papua New Guinea (Momigliano and Utchike 2013); M. sp. from Ogasawara (Reimer et al. 2010); M. sp Psp1-05 from Palau (Carlos et al. 1999); M. sp. D1.1 from Marginopora in Guam (Pochon et al 2007)).\n",
        "\n",
        "In the Symbiodatabaceae db, Foraminifera_D is present, but only two sequences:\n",
        "Foraminifera_D;D1.1_AM748564 (Pochon et al., 2007) and Foraminifera_D;D1.2_AM748617 (Pochon et al., 2007).\n",
        "\n",
        "Additional sequences were retrieved from GenBank, aligned with these two sequences already present in the database, and trimmed to be of the same length.GB accession numbers of these additional *Miliolidium *sequences:\n",
        "\n",
        "```\n",
        "MH307660 rt401\n",
        "KC802073 ex_Marginopora_D1Mv1 (Momigliano and Utchike 2013)\n",
        "AB704035 PSP1-05 (Yamashita,H. and Koike,K., 2013)\n",
        "JN558082 D1.2_PSP1-05_clone_2 (Pochon et al., 2012)\n",
        "JQ247049 D1.1 (Pochon et 1l., 2014)\n",
        "```\n",
        "Sequences from Ogasawara island HM042387-HM042392 were not added because they are very short (~200 bp).\n",
        "\n",
        "The trimmed sequences were exported from Geneious prime as `Miliolidium_add_ref_seq.fasta`, and names edited:\n",
        "\n",
        "```\n",
        "sed -i 's/>/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_/g' Miliolidium_add_ref_seq.fasta # add taxonomy\n",
        "```\n",
        "\n",
        "Edit the original db file to change the genus name and \"species\" name:\n",
        "```\n",
        "sed -i 's/Foraminifera_D/Miliolidium/g' ITS2dbn1_Dinophy_Symbio.fasta\n",
        "sed -i 's/D1.2_AM748617/M_AM748617_D1.2/g' ITS2dbn1_Dinophy_Symbio.fasta\n",
        "sed -i 's/D1.1_AM748564/M_AM748564_D1.1/g' ITS2dbn1_Dinophy_Symbio.fasta\n",
        "```\n",
        "\n",
        "Concatenate two files:\n",
        "```\n",
        "cat ITS2dbn1_Dinophy_Symbio.fasta Miliolidium_add_ref_seq.fasta > ITS2db_M.fasta\n",
        "```\n",
        "\n",
        "In the description paper, it is said that \"The entity reported by Fujise et al. (2020) as â€œforaminifera Dâ€ does not correspond to *Miliolidium*, but appears to represent the discovery of a new divergent symbiodiniacean lineage\". [These sequences](https://github.com/nitschkematthew/Free_living_Symbiodiniaceae_HI/blob/master/R/community_analysis/alignments/Foraminifera_D_aligned.fasta) were compared to our curated ASVs with blastn (-outfmt 6 -evalue 0.0001 -max_target_seqs 3), but no matches > 80% identity and > 200 bp were produced. Since this is the case (and these sequences also don't have a proper genus assignment), we did not include them in the database even though they potentially represent a separate divergent clade of Symbiodiniaceae.\n",
        "\n",
        "<details>\n",
        "  <summary>Miliolidium_add_ref_seq.fasta[click to show details]</summary>\n",
        "\n",
        "```\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_JN558082_D1.2_PSP1-05_clone_2\n",
        "AACCAATGGCCTCCTGAACGTGCATTGCACTCTTGGGATTTCCTGAGAGTATGTTTGCTTCAGTGCTTAGTTTGGTCAACTTTGTTTGGATCCTGCTTCTGGGAACGAAGCACCTCTGTGAATCATTGGAACAAACAGGACTTCGTCTCTGTTTTGCCAATGGTTTGCTTGTCTCTTCAATCCTTTGTGTAGCACAAGCATATGTGCGTTGTTGTTTGATACATTTATATCTCTTTGTAGTATAGATGCTATCTGCACGAAGCACATCTGATATGCTTGCTACTGCTATTGTGCTGCTGAGAATTCTTATATGCATATGTACCATGTGTATATGATGTCTATATGATTTCTCTCCCTTGCACAACCCATAGCATGAAGTCAAACAAGAGA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_JQ247049_D1.1\n",
        "AACCAATGGCCTCCTGAACGTGCATTGCACTCTTGGGATTTCCTGAGAGTATGTTTGCTTCAGTGCTTAGTTTGGTCAACTTTGTTTGGATCCTGCTCCTGGGAACTGAGCGCCTCTGTGAATCATTGGAGCAAACAGGACTTTGTCTCTGTTTTTTTGCCAATGGTTTGCTTGTCTCTACAATCCTTTGTGTAGCGCAAGCATATGTGCGTTGTTGTTTGATACATTTATCTCTTTGTAGCATAAATGCTTTCTGCACGAAGCACATTGATATGCTTGCTACTGCTACTGTGCTGCGGAGAGTTGGTTTTTGCATGTGTACCAGGTGTATAGTATGCTTACACTCTCTTGCACAACCCATAGCATGAAGTCAAACAAGAGA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_AB704035_PSP1-05\n",
        "AACCAATGGCCTCCTGAACGTGCATTGCACTCTTGGGATTTCCTGAGAGTATGTTTGCTTCAGTGCTTAGTTTGGTCAACTTTGTTTGGACCCTGCTTCTGGGAACCAAGCACCTCTGTGAATCATTGGAACAAACAGGACTTCGTCTCTGTTTTGCCAATGGTTTGCTTGTCTCTTCAATCCTTTGTGTAGCACAAGCATATGTGCGTTGTTGTTTGATACATTTATATCTCTTTGTAGTATAGATGCTATCTGCACGAAGCACATCTGATATGCTTGCTACTGCTATTGTGCTGCTGAGAATTCTTATATGCATATGTACCATGTGTATATGATGTCTATATGATTTCTCTCCCTTGCACAACCC\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_KC802073_ex_Marginopora_D1Mv1\n",
        "GACGTGCATTGCACTCTTGGGATTTCCTGAGAGTATGTTTGCTTCAGTGCTTAGTTTGGTCAACTTTGTTTGGATCCTGCTTCTGGGAACCAAGCGCTTTTGTGAACCATTGGAACAGGGAGGAACAACTTCTTCGCTGTTGTGCCAATGGTTTGCTTGTATCTTCAACCTTTTTGGGGTAGCACAAGCATATGTGCGCTGTTGTTTGATGCATTTATCTCTTTCTAGTGGTGAATGCTATCTTGCACGAAGCACATATATATGCTTGCTACTGCTATTGTGCTGCTGAGAGTTGTTATATGCATGTGTACACCATGTGTATATTATGTCTATATGATTACTCTCTCGCACAACCCATAGCATGAAGTCAAACAAGAGA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Miliolidium;M_MH307660_rt401\n",
        "AACCAATGGCCTCCTGAACGTGCATTGCACTCTTGGGATTTCCTGAGAGTATGTTTGCTTCAGTGCTTAGTTTGGTCAACTTTGTTTGGATCCTGCTTCTGGGAACCAAGCACTTTTGTGAACCATTGGAACAGAGAGAGAGAGGGAGAGAGAGAGAACTTCTTCTGCAACTCCTCTGTTTTNCCAATGGTTTGCTTGTATCTCAACCTTTGGTGTAGCACAAGCATATGTGCGCTGTTGTTTGATGCATTTATCTCTTTGTGGTATAAATGCTATCTTGCACCAAGCACATCTTTATGCTTGCTACTGCTATTGTGCTGCTGAGAGTTGTTATATGCATATGTACACCATGTGTATATTATGTCTATATGATTACTCTCTTGCACAACCCATAGCATGAAGTCAAACAAGAGA\n",
        "```\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "RkmWM3Bbasen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add clade J sequences"
      ],
      "metadata": {
        "id": "kANkDn1orIsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clade J sequences were downloaded from GenBank based on the numbers provided in [Yorifuji et al., 2021](https://www.sciencedirect.com/science/article/pii/S1055790321000919), trimmed based on the alignment with clade I, output in fasta format, renamed to include taxonomy, and added to the database file.\n",
        "\n",
        "```\n",
        "cat ITS2db_M.fasta clade_J_add_ref_seq.fasta > ITS2db_MJ.fasta\n",
        "```\n",
        "\n",
        "LC503502.1_OKTs7_13, LC503503.1_OKTs6_09, LC503504.1_OKTs1_03, and LC503505.1_OKTs6_13 were not included because their aligned sequence part was too short.\n",
        "\n",
        "<details>\n",
        "  <summary>clade_J_add_ref_seq.fasta [click to show]</summary>\n",
        "```\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Clade_J;J_LC503409_17OKTs6-04\n",
        "AACCAATGGCCTCCTGAACGCATATTGCACTCTTGGGGTTTCCTGAGAGTATGTCTGCTTCAGTGCTTAGCACATTCTACCCCATGCAAGGCGCGGTGTGATGACGTCTTGTGCTCCTATAAGCCATTCAGAGAGTAGTTGGTGGCTTATCGAGTAGCAGGCTGCAGCGCAAGCTCGTGTGCTTTGTTGTTCATTGCTACCTCCATCCTGTGGGGGTTGCTTTTGACTGCTGCTGATGCTTGCGACCGCTCGGCGCGTGGCGCAAGGAACGTCGCGCTTAT\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Clade_J;J_LC503421_17OKTs7-10\n",
        "AACCAATGGCCTCCTGAACGCATATTGCACCCTTGGGGTTTCCTGAGAGTATGTCTGCTTCAGTGCTTAGCACATTCTACCCCATGCAAGGCGCGGTGTGATGACGTCTTGTGCTCCTATAAGCCATTGAGAGAGTAGTTGGTGGCTTATCGAGTAGCAGGCTGCAGCGCAAGCTCGTGTGCTTTGTTGTTCATTGCTACCTCCATCCTGTGGGGGTTGCTTTTGACCGCTGCTGATGCTTGCGACCACTCGGCGCGTGGAACTTTGCGCTTCT\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Clade_J;J_LC503416_17OKTs7-03\n",
        "AACCAATGGCCTCCTGAACGCATATTGCACTCTTGGGGTTTCCTGAGAGTATGTCTGCTCCAGTGCTTAGCACATTCTACCCCATGCAAGGCGCGGTGTGATGACGTCTTGTGCTCCTATAAGCCATTGAGAGAGTAGTTGGTGGCTTATCGAGTAGCAGGCTGCAGCGCAAGCTCGTGTGCTTTGTTGTTCATTGCTACCTCCATCCTGTGGGGGTTGCTTTTGACCGCTGCTGATGCTTGCGACCACTCGGCGCGTGGAACTTTGCGCTTCT\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Clade_J;J_LC503420_17OKTs7-09\n",
        "AACCAATGGCCTCCTGAACGCATATTGCACTCTTGGGGTTTCCTGAGAGTATGTCTGCTTCAGTGCTTAGCACATTCTACCCCATGCAAGGCGCGGTGTGATGACGTCTTGTGCTCCTATAAGCCATTGAGAGAGTAGTTGGTGGCTTATCGAGTAGCAGGCTGCAGCGCAAGCTCGTGTGCTTTGTTGTTCATTGCTACCTCCATCCTGTGGGGGTTGCTTTTGACCGCTGCTGATGCTTGCGACCACTCGGCGCGTGGAACTTTGCGCTTCT\n",
        "```\n",
        "</details>\n",
        "\n",
        "Clean up\n",
        "```\n",
        "> mv ITS2dbn1_Dinophy_Symbio.fasta Miliolidium_add_ref_seq.fasta clade_J_add_ref_seq.fasta ./db_used/\n",
        "> rm ITS2db_M.fasta Symbiodiniaceae_names.txt\n",
        "> ls\n",
        "ITS2db_MJ.fasta  SymPortal_DIVs_all_clean.fasta  db_used\n",
        "```"
      ],
      "metadata": {
        "id": "r1-XltrC3k-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check clade F genus assignment"
      ],
      "metadata": {
        "id": "aMazzn8M-odp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During one of the test runs with Symbiodatabaceae db, many *Amphisorus* samples showed mixed *Fugacium* and *Freudenthalidium* infections, which warrants further check if the database sequences are all named correctly.\n",
        "\n",
        "Get all the clade F (*Freudenthalidium*, *Fugacium*, clade_Fr2, clade_Fr4) sequences from ITS2db_MJ.fasta:\n",
        "\n",
        "```\n",
        "> grep \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium\" ITS2db_MJ.fasta > Freudenthalidium_ids.txt\n",
        "> grep \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium\" ITS2db_MJ.fasta > Fugacium_ids.txt\n",
        "> grep \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr\" ITS2db_MJ.fasta > clade_Fr2_Fr4_ids.txt\n",
        "> cat Freudenthalidium_ids.txt Fugacium_ids.txt clade_Fr2_Fr4_ids.txt > F_ids.txt\n",
        "> sed -i 's/>Eukaryota/Eukaryota/g' F_ids.txt\n",
        "> seqkit grep -f F_ids.txt ITS2db_MJ.fasta -o ITS2db_F_only.fasta\n",
        "```\n",
        "Clade F definitions:\n",
        "\n",
        "Freudenthalidium \"probably encompasses all previous records of ITS2 types beginning with the F3 identifier\" [Nitschke et al.,  2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/jpy.12999)\n",
        "```\n",
        "P2D2, P2D7, P5G9\tFreudenthalidium\theronense\t   Fr3\tF3.7\tMT025195\n",
        "P1G6\t            Freudenthalidium\tendolithicum\tFr3\tF3.8\tMT025196\n",
        "```\n",
        "Fugacium is described in [LaJeunesse et al., 2018](https://www.sciencedirect.com/science/article/pii/S0960982218309072) as Fr5 (F5):\n",
        "\n",
        "```\n",
        "Fugacium LaJeunesse gen. nov.â€”Formerly â€œSub-cladeâ€ Fr5 within Clade F; Type Species: F. kawagutii\n",
        "This genus comprises species found in foraminifera and several non-symbiotic species known only from cultured isolates. They probably occur at transient, low-abundance densities in cnidarians, but their ecological attributes remain unknown. It is distinct from other â€œClade Fâ€ lineages such as Fr2, Fr3, and Fr4; each of these lineages should be considered separate (as yet undescribed) genera. The name means â€œephemeral.â€\n",
        "```\n",
        "\n",
        "ITS2 sequences belonging to clade F `TS2db_F_only.fasta` were imported into Geneious Prime and aligned in order to check genus assignment. In order to visualize sequence similarity, a phylogenetic tree was built with RAxML 8.2.11* using GTR GAMMA model with Rapid hill-climbing algorithm, Number of starting trees or bootstrap replicates 100, and Parsimony random seed 1. The tree was viewed in Geneious Prime in \"Alignment view\".\n",
        "\n",
        "Reference: Stamatakis A. RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies. Bioinformatics. 2014 May 1;30(9):1312-3."
      ],
      "metadata": {
        "id": "pExdNeGj-sY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several clades can be seen in the resulting tree, and some contain mislabeled sequences.\n",
        "\n",
        "Group1 - clade_Fr2 (F2) - OK\n",
        "```\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr2;F2_AJ830908\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr2;F2b_AJ291521|F2b_AJ291522|F2b_AJ291524\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr2;F2b_AJ291523\n",
        "```\n",
        "\n",
        "Group2 - Freudenthalidium (F3) - contains mislabeled `Fugacium;F_AJ291525`\n",
        "```\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.3_AM748569\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.4a_AM748571\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.4_AM748570\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.4b_AM748616\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.8_MT025196\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.7_MT025195\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.2_AM748567\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.2a_AM748568\n",
        "# Sth went wrong\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AJ291525\n",
        "# All good again\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.1a_AM748566\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3.1_AM748565\n",
        "```\n",
        "\n",
        "Group 3 Fugacium (F5) - Genus OK but some labels are just F or F1, not F5.1/F5.2. In particular, Fugacium;F_AF333516 should be F5.1; Fugacium;F_AF180130 and Fugacium;F1_AF184946|F1_AF333517 should be F5; Fugacium;F_AJ291534 should be F5.2\n",
        "```\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.1c_AM748593\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AF333516\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.1_AM748592|F5.1_AM748615\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.1b_AJ291535\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.1a_AM748591\n",
        "# F5.1 ends\n",
        "# F5.2 starts\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AF180130\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F1_AF184946|F1_AF333517\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2f_AJ291512\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AJ291534\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2e_AJ291533\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2d_AJ291530|F5.2d_AJ291531|F_AJ311945\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2c_AM748596\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2a_AJ291532\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2b_AM748595|F5.2b_AM748613\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5.2_AM748594|F5.2a_AM748614\n",
        "```\n",
        "\n",
        "Group 4 - clade_Fr4 (F4) - contains mislabeled `Fugacium;F_AJ291526` and `Fugacium;F_AJ291528`\n",
        "\n",
        "```\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.5_AM748585\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.4a_AM748583\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.4b_AM748584\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.4_AM748582\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.3c_AM748581\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.3a_AM748579\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.3b_AM748580\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.3_AM748578\n",
        "# Sth went wrong\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AJ291526\n",
        "#All good again\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.7_AM748587\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.6_AM748586\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.1a_AM748573\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.1_AM748572\n",
        "# Sth went wrong\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F_AJ291528\n",
        "#All good again\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.2b_AM748576\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.2a_AM748575\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.2c_AM748577\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.2_AM748574\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8_AM748588\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8b_AM748590\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8a_AM748589\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8c_AJ621136|F4.8c_AJ621137|F4.8c_AJ621138|F4.8c_AJ621139|F4.8c_AJ621140|F4.8c_AJ621142|F4.8c_AJ621143|F4.8c_AJ621144|F4.8c_AJ621145\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8c_AJ291527\n",
        "Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8c_AJ621135|F4.8c_AJ621141\n",
        "```"
      ],
      "metadata": {
        "id": "ZGlk5dYxK6cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label the rebellious *Fugacium*s correctly\n",
        "\n",
        "Freudenthalidium (F3) - contains mislabeled Fugacium;F_AJ291525\n",
        "```\n",
        "> sed -i 's/Fugacium;F_AJ291525/Freudenthalidium;F3_AJ291525/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F_AJ291526/clade_Fr4;F4_AJ291526/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F_AJ291528/clade_Fr4;F4_AJ291528/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F_AF333516/Fugacium;F5.1_AF333516/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F_AF180130/Fugacium;F5_AF180130/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F1_AF184946|F1_AF333517/Fugacium;F5_AF184946|F5_AF333517/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/Fugacium;F_AJ291534/Fugacium;F5.2_AJ291534/g' ITS2db_MJ.fasta\n",
        "```\n",
        "\n",
        "Now `ITS2db_MJ.fasta` contains properly labeled cladeF.\n",
        "\n",
        "No alignment-based check of other clades was performed at this stage."
      ],
      "metadata": {
        "id": "mNlkNPJ_O37t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrate with SymPortal db"
      ],
      "metadata": {
        "id": "g1FE7M6YUa2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identify how many sequences are identical between the databases.\n",
        "\n",
        "Since SymPortal db uses different primers from the Symbiodatabaceae db, the lengths of the sequences will be different between the two files, and even if sequences come from the same source, they will not be 100% identical because of the length.\n",
        "\n",
        "Split ITS2db_MJ.fasta into Symbiodiniaceae and non-Symbiodiniaceae sequences\n",
        "```\n",
        "# clean up the file even more to remove spaces in seq ids\n",
        "> sed -i 's/C1#_JF298202|C#_JF298202 /C1#_JF298202|C#_JF298202/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/C164_KJ801947 /C164_KJ801947/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/C164_KJ801947 /C164_KJ801947/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/C1b_AY239363|C1e_AY258489|C42a_FJ529656 /C1b_AY239363|C1e_AY258489|C42a_FJ529656/g' ITS2db_MJ.fasta\n",
        "> sed -i 's/C35_EU808001|C35 (type 3)_EF541145/C35_EU808001|C35_type_3_EF541145/g' ITS2db_MJ.fasta\n",
        "# write Symbiodiniacese ids in the separate file\n",
        "> grep \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae\" ITS2db_MJ.fasta > ITS2db_Symbio_ids.txt\n",
        "# remove \">\"\n",
        "> sed -i 's/>Eukaryota/Eukaryota/g' ITS2db_Symbio_ids.txt\n",
        "# grep all Symbiodiniaceae seqs\n",
        "> seqkit grep -f ITS2db_Symbio_ids.txt ITS2db_MJ.fasta -o ITS2db_Symbio_only.fasta\n",
        "# grep all non-Symbiodiniaceae seqs\n",
        "> seqkit grep -f ITS2db_Symbio_ids.txt ITS2db_MJ.fasta -v -o ITS2db_non-Symbio_only.fasta\n",
        "```\n",
        "\n",
        "Separate SymPortal db into named sequences and sequences specific for this dataset:\n",
        "\n",
        "```\n",
        "> cat SymPortal_DIVs_all_clean.fasta |grep \"_\" | cut -c 2- > SymPortal_dataset_specific_ids.txt\n",
        "> seqkit grep -f SymPortal_dataset_specific_ids.txt SymPortal_DIVs_all_clean.fasta -o SymPortal_dataset_specific.fasta\n",
        "> seqkit grep -f SymPortal_dataset_specific_ids.txt SymPortal_DIVs_all_clean.fasta -v -o SymPortal_named.fasta\n",
        "```\n",
        "\n",
        "Blast SymPortal sequences agains ITS2db_Symbio\n",
        "\n",
        "```\n",
        "# blastn: 2.12.0+\n",
        "makeblastdb -in ./ITS2db_Symbio_only.fasta -dbtype 'nucl'  # make blast database\n",
        "\n",
        "blastn -num_threads 6 -task blastn -db ITS2db_Symbio_only.fasta -query SymPortal_dataset_specific.fasta -out ./SymPortal_specific_against_ITS2db.out -outfmt 6 -evalue 0.0001 -max_target_seqs 3\n",
        "\n",
        "blastn -num_threads 6 -task blastn -db ITS2db_Symbio_only.fasta -query SymPortal_named.fasta -out ./SymPortal_named_against_ITS2db.out -outfmt 6 -evalue 1e-80 -max_target_seqs 5\n",
        "```"
      ],
      "metadata": {
        "id": "KPRUOnZiW09E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the blast table contains all the SymPortal db entries:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "> cat SymPortal_DIVs_all_clean.fasta | grep \"^>\" | sort | uniq | wc -l\n",
        "6017 # total Symportal unique ids\n",
        "> cat SymPortal_dataset_specific.fasta | grep \"^>\" | sort | uniq | wc -l\n",
        "515\n",
        "> cat SymPortal_named.fasta | grep \"^>\" | sort | uniq | wc -l    \n",
        "5502\n",
        "> cat  SymPortal_specific_against_ITS2db.out | cut -f 1 | sort | uniq | wc -l\n",
        "515\n",
        "> cat SymPortal_named_against_ITS2db.out | cut -f 1 | sort | uniq | wc -l\n",
        "5438 # 64 named sequences from SymPortal db did not have a good blast match\n",
        "# find out which sequences don't have matches\n",
        "> cat SymPortal_named_against_ITS2db.out | cut -f 1 | sort | uniq > blast_named_matches.txt\n",
        "> cat SymPortal_named.fasta | grep \"^>\" | cut -c 2- | sort | uniq > SymPortal_named_ids.txt\n",
        "> grep -F -x -v -f blast_named_matches.txt SymPortal_named_ids.txt\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lo4fsnDOzx2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename SymPortal entries\n",
        "For those SymPortal named variants, that do not have direct match in the ITS2db, generate a fasta file with taxonomical names in order to add them to the database\n",
        "\n",
        "```\n",
        "# Produce a file with named sequences that have a good match of 100% identity and >200 base pairs\n",
        "> cat SymPortal_named_against_ITS2db.out | awk '$3==100 && $4>200' | cut -f 1 | sort | uniq > perfect_match_ids.txt\n",
        "# Get ids of those which do not have perfect match\n",
        "# blast_named_matches.txt are all blastn matches\n",
        "# perfect_match_ids.txt are 100% matches\n",
        "> grep -F -x -v -f perfect_match_ids.txt blast_named_matches.txt > no_perfect_match_ids.txt\n",
        "# Get fasta file with those SymPortal seqs that did not have perfect match to the ITS2db\n",
        "> seqkit grep -f no_perfect_match_ids.txt SymPortal_DIVs_all_clean.fasta -o no_perfect_match.fasta\n",
        "[INFO] 4952 patterns loaded from file\n",
        "# Generate taxonomically informative names by replacing parts of the names with full taxonomy strings\n",
        "> sed -i 's/>A/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Symbiodinium;A/g' no_perfect_match.fasta\n",
        "> sed -i 's/>D/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Durusdinium;D/g' no_perfect_match.fasta\n",
        "> sed -i 's/>C/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;C/g' no_perfect_match.fasta\n",
        "> sed -i 's/>F3/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Freudenthalidium;F3/g' no_perfect_match.fasta\n",
        "> sed -i 's/>F5/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5/g' no_perfect_match.fasta\n",
        ">  sed -i 's/>I/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_I;I/g' no_perfect_match.fasta\n",
        "> sed -i 's/>B/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Breviolum;B/g' no_perfect_match.fasta\n",
        "> sed -i 's/>G/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Gerakladium;G/g' no_perfect_match.fasta\n",
        "> sed -i 's/>F2/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr2;F2/g' no_perfect_match.fasta\n",
        "> sed -i 's/>F1/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_F1;F1/g' no_perfect_match.fasta\n",
        "> sed -i 's/>F4/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4/g' no_perfect_match.fasta\n",
        "> sed -i 's/>E1/>Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Effrenium;E/g' no_perfect_match.fasta\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jX3-Yjk3-R6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments regarding clade renaming:\n",
        "\n",
        "**Clade A.**\n",
        "Although clade A also includes *Philozoon*, all the clade A variants here were renamed as *Symbiodinium*. Partially this is because the only information about *Philozoon* ITS2 variant found in the description paper ([LaJeunesse et al., 2021](https://www.tandfonline.com/doi/full/10.1080/09670262.2021.1914863#abstract)\n",
        ") was A19 ITS2 variant not present in the current dataset, partially because *Philozoon *is temperate clade while our samples come from Okinawa.\n",
        "\n",
        "**Clade B.**\n",
        "All renamed as *Breviolum*. No additional genera are present to our knowledge.\n",
        "\n",
        "**Clade C.**\n",
        "All renamed as *Cladocopium*. No additional genera are present to our knowledge.\n",
        "\n",
        "**Clade D.**\n",
        "Includes *Durusdinium* and Foraminifera clade D (*Miliolidium*).\n",
        "However, in our blast results against ITS2db_Symbio_only.fasta, only the following genera are present:\n",
        "\n",
        "```\n",
        "> cat SymPortal_named_against_ITS2db.out |cut -f 2 | cut -d \";\" -f 6 | sort | uniq\n",
        "Breviolum\n",
        "Cladocopium\n",
        "Durusdinium\n",
        "Effrenium\n",
        "Freudenthalidium\n",
        "Fugacium\n",
        "Gerakladium\n",
        "Halluxium\n",
        "Symbiodinium\n",
        "clade_Fr2\n",
        "clade_Fr4\n",
        "clade_I\n",
        "```\n",
        "*Miliolidium* was not present in the blast results, meaning there were no close matches to this genus. So all variants starting with D were renamed to *Durusdinium*.\n",
        "\n",
        "**Clade E** All renamed as *Effrenium*. No additional genera are present to our knowledge.\n",
        "\n",
        "**Clade F** Was renamed to *Fugacium* if starting with F5, *Freudenthalidium* if starting with F3, and clade_Fr2 if starting with F2, clade clade_Fr4 if starting with F4. In addition, several sequences were labeled as F1... They wre renamed as clade_F1.\n",
        "\n",
        "**Clade G**. Should be split into two genera, Foraminifera G and *Gerakladium*. However, since this split was not done in the original database and almost no clade G sequences are present in our environmental and host dataset according to SymPortal, no such division was inplemented here.\n",
        "\n",
        "**Clade H.** All the matches of named variants were 100% match (already present in ITS2db). No renaming was done."
      ],
      "metadata": {
        "id": "fSaqEMP0-Wo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two problems left with ITS2_db file:\n",
        "\n",
        "1) Many of the entries from original ITS2db have no proper subclade (such as C1, C3, etc.) labeling. For example, C_EU074881 or D_EF656435 or H_AJ291513 or A_AF201746. Is this A closer to A3, A6, or something completely different? Some of these entries even have perfect matches within SymPortal ITS2 database (C_DQ480613 has 100% identity over 263 to C1). However, replacing such entries with their SymPortal counterparts would mean loosing information about flanks (SymPortal db is shorter) and about GenBank accession numbers. In addition, some entries might not have a 100% match but still be close enough to one or another variant for this information to be useful.\n",
        "\n",
        "\n",
        "2) It does not include ITS2 variants from our dataset (environmental and host samples) that SymPortal has already identified during the previous runs."
      ],
      "metadata": {
        "id": "YJzBZeehEFtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename ITS2db entries adding SymPortal hits\n",
        "\n",
        "Run blastn using SymPortal named variants as database.\n",
        "\n",
        "```\n",
        "# blastn: 2.12.0+\n",
        "makeblastdb -in ./SymPortal_named.fasta -dbtype 'nucl'  # make blast database\n",
        "\n",
        "blastn -num_threads 6 -task blastn -db SymPortal_named.fasta -query ITS2db_Symbio_only.fasta -out ./ITS2db_against_SymPortal_named.out -outfmt 6 -evalue 0.0001 -max_target_seqs 2\n",
        "```\n",
        "The resulting file `ITS2db_against_SymPortal_named.out` contains 2 best hits for every entry in ITS2db_Symbio_only.\n",
        "Some of the hits are not great quality, for example, for clade J.\n",
        "\n",
        "```\n",
        "# filter out low id and short reads\n",
        "> cat ITS2db_against_SymPortal_named.out | awk '$3>96 && $4>200' > ITS2db_against_SymPortal_named_filt.out\n",
        "```\n",
        "Now, I want to do the following with each of the ITS2db ids:\n",
        "1) make shorter if it has an excessively long string of GB accession numbers (some have more than 10)\n",
        "2) append the name of the best matches from SymPOrtal named variants."
      ],
      "metadata": {
        "id": "4FZ6OCnGGzDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce a table with old_id-tab-new_id"
      ],
      "metadata": {
        "id": "nn1ri-D1Py6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file in Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# upload ITS2db_against_SymPortal_named_filt.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Cxjlgp7IbM3I",
        "outputId": "2a907d63-cd89-4917-f7ee-49f440c417ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74b8648b-6ba6-46df-80bd-cad0d40ea257\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74b8648b-6ba6-46df-80bd-cad0d40ea257\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ITS2db_against_SymPortal_named_filt.out to ITS2db_against_SymPortal_named_filt.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ITS2db_against_SymPortal_named_filt.out\", 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  ITS2_names_old_to_new = dict()\n",
        "  for entry in lines:\n",
        "    line = entry.split('\\n')[0]\n",
        "    symportal_name = line.split('\\t')[1]\n",
        "    ITS2db_orig_name = line.split('\\t')[0]\n",
        "    identity = round(float(line.split('\\t')[2]), 1)\n",
        "    if identity == 100:\n",
        "      identity = \"\"\n",
        "    else:\n",
        "      identity = \"_\"+str(identity)\n",
        "    if ITS2db_orig_name in ITS2_names_old_to_new:\n",
        "       ITS2db_new_name = ITS2_names_old_to_new[ITS2db_orig_name]+\"_\"+symportal_name+identity\n",
        "       ITS2_names_old_to_new[ITS2db_orig_name] = ITS2db_new_name\n",
        "    else:\n",
        "      if ITS2db_orig_name.count(\"|\") > 1:\n",
        "        ITS2db_new_name = ITS2db_orig_name.split(\"|\")[0]+\"|\"+\"_\"+symportal_name+identity\n",
        "      else:\n",
        "        ITS2db_new_name = ITS2db_orig_name+\"_\"+symportal_name+identity\n",
        "      ITS2_names_old_to_new[ITS2db_orig_name] = ITS2db_new_name\n",
        "\n",
        "\n",
        "for key in ITS2_names_old_to_new:\n",
        "  print(key+\"\\t\"+ITS2_names_old_to_new[key])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3MKDMP6AdKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the correspondence file as `ITS2db_old_to_new_names.txt`\n",
        "\n",
        "The names still are not perfect (there is some redundance such as ...clade_Fr4;F4.3c_AM748581_F4.3c_99.6_F4.3_98.8 has F4.3 repeated 3 times when only one time would suffice). But it should be enough to roughly understand where do the sequences belong to, especially for those sequences that did not have \"subclade\" (for example, A_EF656429_A1cn_A1ip_99.6 or D_AF174571_D1fr_99.6_D1la_99.6 or C_EF428338_C40c_98.6_C3dg_99.6)."
      ],
      "metadata": {
        "id": "F3zWj1lPU8C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the fasta file headers using the table and remove sequences less that 200 bp:\n",
        "\n",
        "```\n",
        "> seqkit replace -p '^(.+)$' -r '{kv}' -k ITS2db_old_to_new_names.txt --keep-key ITS2db_Symbio_only.fasta | seqkit seq --min-len 200 > ITS2db_Symbio_only_new_names.fasta\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3FjLeRMTNYmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenate sequences from ITS2db and SymPortal named sequences\n",
        "```\n",
        "cat ITS2db_Symbio_only_new_names.fasta no_perfect_match.fasta > ITS2db_SymPortal_nonmatch.fasta\n",
        "```\n",
        "\n",
        "Now file `ITS2db_SymPortal_nonmatch.fasta` has all the named sequences from SymPortal: either as themselves (labeled something like as Eukaryota...Durusdinium;D1kt) or as their perfect matches from original ITS2db (originally Eukaryota...Cladocopium;C_EU074881). It has 5726 entries. Some entries did not have a good blast match and don't have SymPortal clade appended in the end, but it's very few.\n",
        "\n",
        "Clean up - remove all the files that are easy to reproduce, move the files that are good to keep as intermediate\n",
        "\n",
        "```\n",
        "> mkdir used\n",
        "> mv ITS2db_MJ.fasta ./db_used/\n",
        "> mv ITS2db_old_to_new_names.txt used\n",
        "> mv SymPortal_DIVs_all_clean.fasta db_used/\n",
        "> mv SymPortal_named.fasta db_used/\n",
        "> mv ITS2db_Symbio_only.fasta db_used/\n",
        "> mv no_perfect_match.fasta used/\n",
        "> rm *fasta.* # rm blast databases - dot after fasta is important\n",
        "> rm ITS2db_Symbio_only_new_names.fasta\n",
        "> rm *txt\n",
        "> rm *out\n",
        "> ls\n",
        "ITS2db_SymPortal_nonmatch.fasta # Symbiodiniaceae sequences from ITS2db plus SymPortal named variants\n",
        "ITS2db_non-Symbio_only.fasta # non-Symbiodiniaceae sequences from ITS2db\n",
        "SymPortal_dataset_specific.fasta # dataset-specific sequences only from SymPortal (such as 5621_F or 5718_C)\n",
        "db_used\n",
        "trees\n",
        "used\n",
        "```"
      ],
      "metadata": {
        "id": "W-7AKbRvDU0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rename dataset-specific sequences from SymPortal\n",
        "\n"
      ],
      "metadata": {
        "id": "55PNlx7rYZBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SymPortal dataset-specific sequences do not offer resolution below Genus, and sometimes even Family (_F which can mean *Fugacium*, *Freudenthalidium*, Fr2 or Fr4) level. Even the alignments provided by SymPortal among the results (for example, 20250805T070924_all\\between_sample_distances\\F\\clade_F_seqs.aligned.fasta) do not have the reference sequences even if those reference sequences were identified in the dataset. Therefore, blastn will be used to identify closest match of those dataset-specific sequnces in the database ITS2db_SymPortal_nonmatch.fasta.\n",
        "\n",
        "```\n",
        "# blastn: 2.12.0+\n",
        "> makeblastdb -in ./ITS2db_SymPortal_nonmatch.fasta -dbtype 'nucl'  # make blast database\n",
        "\n",
        "> blastn -num_threads 6 -task blastn -db ITS2db_SymPortal_nonmatch.fasta -query SymPortal_dataset_specific.fasta -out ./Symportal_specific_against_ITS2db.out -outfmt 6 -evalue 0.0001 -max_target_seqs 3\n",
        "\n",
        "# examine the number of sequences in the fasta file\n",
        "> cat SymPortal_dataset_specific.fasta | grep \"^>\" | wc -l\n",
        "515 # unnamed sequences specific for our dataset\n",
        "> cat Symportal_specific_against_ITS2db.out | cut -f 1 | sort | uniq | wc -l\n",
        "515 # queries found their blast matches  \n",
        "# filter put short alignments\n",
        "> cat Symportal_specific_against_ITS2db.out | awk '$4>196' > Symportal_specific_against_ITS2db_minlen197.out\n",
        "> cat Symportal_specific_against_ITS2db_minlen197.out | cut -f 1 | sort | uniq | wc -l\n",
        "515\n",
        "```\n",
        "\n",
        "How many of the sequences don't have a close match?\n",
        "```\n",
        "cat Symportal_specific_against_ITS2db_minlen197.out | awk '$3<90' | cut -f 1 | sort | uniq | wc -l\n",
        "# 46 out of 515\n",
        "```\n",
        "What clades do they belong to?\n",
        "\n",
        "```\n",
        "> cat Symportal_specific_against_ITS2db_minlen197.out | awk ''$3<90' | cut -f 1 | gr\n",
        "ep \"_A\" | uniq | wc -l\n",
        "35\n",
        "> cat Symportal_specific_against_ITS2db_minlen197.out | awk '$3<90' | cut -f 1 | grep \"_F\" | uniq | wc -l\n",
        "10\n",
        "> cat Symportal_specific_against_ITS2db_minlen197.out | awk '$3<90' | cut -f 1 | grep \"_C\" | uniq | wc -l\n",
        "1\n",
        "```\n",
        "35 clade A sequences, 10 clade F sequences, and 1 clade C sequences don't have a match with identity more than 90%."
      ],
      "metadata": {
        "id": "P1feqQnzjr00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the sequences, appending the \"species\" name of the best match and identity to this match"
      ],
      "metadata": {
        "id": "2sr3L8nPrqvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file in Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# upload Symportal_specific_against_ITS2db_minlen197.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "2bAsG4yjtZ7D",
        "outputId": "af5a31e2-0d89-40c9-a415-855fc100b356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95819623-5fa2-4dd5-8e9a-60b771f0b7a4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-95819623-5fa2-4dd5-8e9a-60b771f0b7a4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Symportal_specific_against_ITS2db_minlen197.out to Symportal_specific_against_ITS2db_minlen197.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Symportal_specific_against_ITS2db_minlen197.out\", 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  names_old_to_new = dict()\n",
        "  for entry in lines:\n",
        "    line = entry.split('\\n')[0]\n",
        "    symportal_id = line.split('\\t')[0]\n",
        "    ITS2db_name = line.split('\\t')[1]\n",
        "    genus = ITS2db_name.split(';')[5]\n",
        "    species = ITS2db_name.split(';')[6]\n",
        "    prefix = \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;\"\n",
        "    identity = round(float(line.split('\\t')[2]), 1)\n",
        "    if identity == 100:\n",
        "      identity = \"_id\"+str(int(identity))\n",
        "    else:\n",
        "      identity = \"_id\"+str(identity)\n",
        "\n",
        "    if symportal_id in names_old_to_new:\n",
        "      pass\n",
        "      #print(\"name already in dict:\", symportal_id)\n",
        "      #print (\"match \", identity, genus, species)\n",
        "    else:\n",
        "      #print (\"name not in dict:\", symportal_id)\n",
        "      #print(\"match \", identity, genus, species)\n",
        "      symportal_id_new = prefix+genus+\";\"+symportal_id+identity+\"_\"+species\n",
        "      #print(\"new id\", genus+\";\"+symportal_id+identity+\"_\"+species)\n",
        "      names_old_to_new[symportal_id] = symportal_id_new\n",
        "\n",
        "\n",
        "for key in names_old_to_new:\n",
        "  print(key+\"\\t\"+names_old_to_new[key])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vERphm_ltkTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save output as `SymPortal_specific_names_old_to_new.txt`\n",
        "\n",
        "Rename the sequence headers in fasta file\n",
        "\n",
        "```\n",
        "> seqkit replace -p '^(.+)$' -r '{kv}' -k SymPortal_specific_names_old_to_new.txt --keep-key SymPortal_dataset_specific.fasta > SymPortal_dataset_specific_new_names.fasta\n",
        "```\n"
      ],
      "metadata": {
        "id": "Q-QR0awOyBvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenate SymPortal unnamed and ITS2db Symbio\n",
        "\n",
        "```\n",
        "cat ITS2db_SymPortal_nonmatch.fasta SymPortal_dataset_specific_new_names.fasta > ITS2db_Symbio_SymPortaldb.fasta\n",
        "```\n",
        "Final checks and statistic:\n",
        "\n",
        "```\n",
        "> cat ITS2db_Symbio_SymPortaldb.fasta | grep \"^>\" | sort | uniq | wc -l\n",
        "6241 # unique Symbiodiniaceae sequences are present\n",
        "# list all the genera\n",
        "> cat ITS2db_Symbio_SymPortaldb.fasta | grep \"^>\" | cut -d \";\" -f 6 | sort | uniq\n",
        "Breviolum\n",
        "Clade_J\n",
        "Cladocopium\n",
        "Durusdinium\n",
        "Effrenium\n",
        "Freudenthalidium\n",
        "Fugacium\n",
        "Gerakladium\n",
        "Halluxium\n",
        "Miliolidium\n",
        "Symbiodinium\n",
        "clade_F1\n",
        "clade_Fr2\n",
        "clade_Fr4\n",
        "clade_I\n",
        "# rename Clade_J to clade_J for esthetical reasons\n",
        "> sed -i 's/Clade_J/clade_J/g' ITS2db_Symbio_SymPortaldb.fasta\n",
        "```\n",
        "\n",
        "Many names are unnecessarily repetitive: for example, ...clade_Fr4;6005_F_id99.2_F4_AJ291528_F4.1_99.2_F4.1a_98.0 states that this sequence belongs to clade F4.1 twice, and the second match is worse than the first match; such matches should be renamed. However, here they are not renamed and redundancy remains."
      ],
      "metadata": {
        "id": "MkJvaDzJTtwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove duplicates\n",
        "```\n",
        "> cat ITS2db_Symbio_SymPortaldb.fasta | seqkit rmdup -s -d duplicated.fasta -D duplicated.detail.txt -o ITS2db_Symbio_SymPortaldb_edited_nodup.fasta\n",
        "\n",
        "[INFO] 1 duplicated records removed\n",
        "2\tEukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;C15_AY239369|_C40c_98.2_C15, Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;C15new2_NA_C40c_98.2_C15\n",
        "```\n"
      ],
      "metadata": {
        "id": "7xTDPD6SUEJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correct non-Symbiodiniaceae ITS2db"
      ],
      "metadata": {
        "id": "9LRZvwK_ULGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Filter out short non-Symbiodiniaceae sequences:**\n",
        "```\n",
        "> cat ITS2db_non-Symbio_only.fasta | seqkit seq --min-len 150 > ITS2db_non-Symbio_minlen150.fasta\n",
        ">  sed -i 's/ /_/g' ITS2db_non-Symbio_minlen150.fasta\n",
        ">  sed -i 's/\\'//g' ITS2db_non-Symbio_minlen150.fasta\n",
        ">  cat ITS2db_non-Symbio_minlen150.fasta | seqkit stat    \n",
        "file  format  type  num_seqs    sum_len  min_len  avg_len  max_len\n",
        "-     FASTA   DNA     11,901  2,758,729      150    231.8    1,572\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "00laRA1Q5DBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Edit incorrect taxonomic names**\n",
        "\n",
        "Rename *Gymnodinium linucheae*\n",
        "\n",
        "`Gymnodiniales;Gymnodiniaceae;Gymnodinium;Gymnodinium_linucheae` is incorrect because *Gymnodinium linucheae* was renamed as *Symbiodinium linucheae *([LaJeunesse et al., 2018](https://www.sciencedirect.com/science/article/pii/S0960982218309072#mmc2), Data S1):\n",
        "\n",
        "\n",
        "> S. linucheae (Trench & Thinh) LaJeunesse, comb.\n",
        "nov.\n",
        "\n",
        ">Basionym: Gymnodinium linucheae Trench & Thinh\n",
        "(1995) European Journal of Phycology 30: 150.\n",
        "Homotypic synonym: Zooxanthella linucheae (Trench &\n",
        "Thinh) Guiry & R.A.Andersen 2018: 3.\n",
        "Circumscription: Here limited to type A4 organisms as\n",
        "defined by ITS2 rDNA genotyping as designated by\n",
        "LaJeunesse [S27].\n",
        "\n",
        "```\n",
        "sed -i 's/Eukaryota;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae;Gymnodinium;Gymnodinium_linucheae/Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Symbiodinium;A4_Symbiodinium_linucheae/g' ITS2db_non-Symbio_minlen150.fasta\n",
        "```\n",
        "\n",
        "Rename *Gymnodinium beii*\n",
        "`Gymnodiniales;Gymnodiniaceae;Gymnodinium;Gymnodinium_beii` is also incorrect as it was renamed as *Pelagodinium*:\n",
        "\n",
        "> Siano, R., Montresor, M., Probert, I., Not, F., & de Vargas, C. (2010). *Pelagodinium *gen. nov. and *P. bÃ©ii* comb. nov., a dinoflagellate symbiont of planktonic foraminifera. Protist, 161(3), 385-399. https://doi.org/10.1016/j.protis.2010.01.002\n",
        "\n",
        "```\n",
        "sed -i 's/Eukaryota;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae;Gymnodinium;Gymnodinium_beii/Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Suessiaceae;Pelagodinium;Pelagodinium_beii/g' ITS2db_non-Symbio_minlen150.fasta\n",
        "```"
      ],
      "metadata": {
        "id": "9r7BQ2qKVh7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Woloszynskia cincta* Siano, Montresor &\n",
        "Zingone in Siano et al. (2009, 54, Figs. 35â€“44) was moved to *Biecheleria cincta* (Siano, Montresor & Zingone) Siano comb. nov. (source: Balzano, S., Gourvil, P., Siano, R., Chanoine, M., Marie, D., Lessard, S., ... & Vaulot, D. (2012). Diversity of cultured photosynthetic flagellates in the northeast Pacific and Arctic Oceans in summer. Biogeosciences, 9(11), 4553-4571. https://doi.org/10.5194/bg-9-4553-2012).\n",
        "\n",
        "According to the original description, *Biecheleriopsis* also belongs to Suessiales and was described from a culture maitained under the name *Gymnodinium corii*:\n",
        "> \"*Biecheleriopsis* is a genus of thin-walled dinoflagellates, related to *Biecheleria* and the taxonomic group of *Polarella*, *Protodinium* and *Symbiodinium* ...\" \"...we became aware of an isolate from the Adriatic Sea maintained at Institut Francais Recherche Exploration de la Mer (IFREMER), Nantes, France under the name *Gymnodinium corii* J. Schiller, and a closer study of this small species showed it to resemble *Biecheleria*. ... Although superficially similar to *Biecheleria*, the isolate differs ultrastructurally and genetically from this genus, meriting the establishment of a new genus, *Biecheleriopsis* gen. nov.\"\n",
        "Source: Moestrup, Ã˜., Lindberg, K., & Daugbjerg, N. (2009). Studies on woloszynskioid dinoflagellates V. Ultrastructure of *Biecheleriopsis* gen. nov., with description of *Biecheleriopsis adriatica* sp. nov. Phycological Research, 57(3), 221-237.  https://doi-org.ezproxy.oist.jp/10.1111/j.1440-1835.2009.00541.x"
      ],
      "metadata": {
        "id": "_zf26eWoPaxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_names_dict=dict()\n",
        "new_names_dict[\"Eukaryota;Dinoflagellata;Dinophyceae;Lophodiniales;Lophodiniaceae;Woloszynskia;Woloszynskia_cincta\"] = \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Biecheleriaceae;Biecheleria;Biecheleria_cincta\"\n",
        "new_names_dict[\"Eukaryota;Dinoflagellata;Dinophyceae;Lophodiniales;Lophodiniaceae;Biecheleriopsis;Biecheleriopsis_adriatica\"] = \"Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Biecheleriaceae;Biecheleriopsis;Biecheleriopsis_adriatica\"\n",
        "for key in new_names_dict:\n",
        "  print(key+\"\\t\"+new_names_dict[key])\n"
      ],
      "metadata": {
        "id": "dT4rZxM0Ld1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save as `ITS2db_nonsym_old_to_new_names.txt `\n",
        "\n",
        "```\n",
        "> seqkit replace -p '^(.+)$' -r '{kv}' -k ITS2db_nonsym_old_to_new_names.txt --keep-key ITS2db_non-Symbio_minlen150.fasta > ITS2db_non-Symbio_minlen150_corrected.fasta\n",
        "```\n",
        "\n",
        "\n",
        "Remove *Gymnodinium corii*:\n",
        "\n",
        "```\n",
        "> echo \"Eukaryota;Dinoflagellata;Dinophyceae;Gymnodiniales;Gymnodiniaceae;Gymnodinium;Gymnodinium_corii\" > Gymnodinium_corii_ids.txt\n",
        "> cat ITS2db_non-Symbio_minlen150_corrected.fasta | seqkit grep -f Gymnodinium_corii_ids.txt -v -o ITS2db_non-Symbio_minlen150_corr.fasta\n",
        "```\n",
        "\n",
        "```\n",
        "> seqkit stat ITS2db_non-Symbio_minlen150_corr.fasta\n",
        "file                                    format  type  num_seqs    sum_len  min_len  avg_len  max_len\n",
        "ITS2db_non-Symbio_minlen150_corr.fasta  FASTA   DNA     11,899  2,758,069      150    231.8    1,572\n",
        "```"
      ],
      "metadata": {
        "id": "lMTsP6I4tvxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenate all Symbiodiniaceae with non-Symbiodiniaceae sequences\n",
        "\n",
        "```\n",
        "> cat ITS2db_non-Symbio_minlen150_corr.fasta ITS2db_Symbio_SymPortaldb_edited_nodup.fasta > ITS2db_SymPortaldb_full.fasta\n",
        "```\n",
        "\n",
        "Check number of sequences\n",
        "```\n",
        "> seqkit stat ITS2db_SymPortaldb_full.fasta\n",
        "file                           format  type  num_seqs    sum_len  min_len  avg_len  max_len\n",
        "ITS2db_SymPortaldb_full.fasta  FASTA   DNA     18,139  4,379,866      150    241.5    1,572\n",
        "```\n",
        "\n",
        "Check duplicates:\n",
        "\n",
        "```\n",
        "> cat ITS2db_SymPortaldb_full.fasta | seqkit rmdup -s -d duplicated.fasta -D duplicat\n",
        "ed.detail.txt  -o test.fasta\n",
        "```\n",
        "\n",
        "57 duplicates exist but all non-Symbiodiniaceae."
      ],
      "metadata": {
        "id": "pGc2phiEVI5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final ITS2 database file"
      ],
      "metadata": {
        "id": "4BVlKW80CjCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ITS2db_SymPortaldb_full.fasta` is the final clean database file. It is based on SymPortal database and a database from [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719). It contains:\n",
        "\n",
        "-  all named Symbiodiniaceae sequences from SymPortal (source: SymPortal_unique_DIVs.fasta at https://symportal.org/)\n",
        "-  Symbiodiniaceae sequences specific for our dataset (sourse: SymPortal run oun our data, see [SymPortal run on all the samples](https://colab.research.google.com/drive/1GRM1S0ym1xwSkT8PYU8Fx7nQJt2hDv1g#scrollTo=Ny5KREbcf0qW)  earlier in this notebook) with names edited to indicate the closest blast match\n",
        "- ITS2 sequences from the whole eukaryotic tree of life via [Symbiodatabaceae](https://github.com/nitschkematthew/Symbiodatabaceae), [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719): ITS2 database http://its2-old.bioapps.biozentrum.uni-wuerzburg.de subsampled to 1 sequence per genus. These sequences were filtered > 150 bp.\n",
        "- ITS2 sequences especially from dinoflagellates via [Symbiodatabaceae](https://github.com/nitschkematthew/Symbiodatabaceae), [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719). *Gymnodinium linucheae* was renamed to *Symbiodinium linucheae*, *Gymnodinium beii* to *Pelagodinium beii*, *Woloszynskia cincta* to *Biecheleria cincta*, `Lophodiniales;Lophodiniaceae;Biecheleriopsis` to `Suessiales;Biecheleriaceae;Biecheleriopsis`, and *Gymnodinium corii* was removed.\n",
        "- ITS2 sequences from Symbiodiniaceae via [Symbiodatabaceae](https://github.com/nitschkematthew/Symbiodatabaceae), [Fujice et al., 2020](https://onlinelibrary-wiley-com.ezproxy.oist.jp/doi/10.1111/mec.15719): GeoSymbio (Franklin et al 2012) and SymbioGBR (Tonk et al 2013). These sequences were renamed compared to original [Symbiodatabaceae](https://github.com/nitschkematthew/Symbiodatabaceae): only first 2 GB accession numbr left; name of the closest blastn match from SymPortal_unique_DIVs.fasta and identity appended.\n",
        "\n",
        "<details>\n",
        "  <summary>Entries examples [click to show details]</summary>\n",
        "\n",
        "ITS2 sequences from the whole eukaryotic tree of life:\n",
        "```\n",
        ">Eukaryota;unknown;Oomycetes;Saprolegniales;Saprolegniaceae;Achlya;Achlya_sp._BKKU1005\n",
        "TCAAACCAATTGTTTCATGTATTTGAAGCAGAATGTGAAGTGTCTTGCTCTTTTGAACAAGTCCTTTTAAATGAAACGTACCTATATACAACGTAAGAAGCATACCAATATAAAGGTGTGTATCATTTGAATTTCAATATTGTTTTGTATGAATATACATGACGTTGAAAGTAAAATGAATTGCGGTAGTTTTGCTTGTACTTGTACAGGTGAACAATATATTGCTTTTTGTTTTACTGTAGATGTGTGTATTGAAATATAAGATAAGAAGTTTATAATGGTATATACTGGGTTGTTTCTGTAGTATATAGAAGCAAATGGGAAATATATCCAATTTGGA\n",
        ">Eukaryota;Streptophyta;Liliopsida;Asparagales;Orchidaceae;Achlydosa;Achlydosa_glandulosa\n",
        "AAGCATTATGTCGCTTCATTCGACACCCCCCGCCAATAAGGTGTGATGGCGTCGGATCGAATGCGGAGAGTGGCCCTTCGTGCACATTTGTGTGACGGGTTGAAGAGCGGATTGCTTTCCTCTTGGCCGTGTTTTGATAAAGGGGTGGATGGATGCTGCCAGTAGGCCCACGCTGTCATAACATTGTCTTGGGGAGGATAGATATACACATCCGTAGTAGATCACCCGATTAATATGTCGTAGGTGACATCTTGAAAT\n",
        "```\n",
        "\n",
        "ITS2 sequences especially from dinoflagellates\n",
        "```\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Gymnodiniales;Kareniaceae;Karlodinium;Karlodinium_veneficum\n",
        "GTGAATTGCAGAATTCCGTGAACCAATAGGGATTTGAACGTATACTGCGCTTTCGGGTTATTCCTGAAAGCATGCCTTCTTCAGTGTCTACTGATTTTCATGCCCCTGACGTATGCCAAGTCTTCTTGGTGCTGTCATGGGTGCGTGTTTGTGTGTCAAGGAGCCTCTTTTGGCCTTTGACGCATTCAGTGTACAGGTAGCGTCTCCAACGAGCAACTTAATAAGCATCTTTCGGTGTCTCTTTCGTTGTGYAGTTGTTGACTCRTAGCCTGCTCCAGCTCACGACTCCTTCGAGAGTTGTCATTCAAGACATGAAGTTAGGTCAGCAAA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Suessiaceae;Pelagodinium;Pelagodinium_sp\n",
        "GTGAATTGCAGAATTCCGTGAACCAATGGATCTTTGAACGTGCATTGCACTCTCGGGATATCCCTGGGAGTATGCCTGCTTCAGTGCTTGTGGAATCCTCATGCAGGTGCTTGTCCATCTTGGATGTGCCTGTGTCTTTGTGTGCCATGGCTCTGTCTTCTTGGCGGAGTCAGTGGCGCATTCAGGGCTAGCGATTGCTCTGGCATCTGCAACTTGTGAAGTACACTCTTGTGCACCTGCTGGTTGTGTTGCCTTAAAGAGCTCTCTGACATAGCTTCGGCTATCTCCTGCACAGCATGAAGTCAGGTCAGCAAACCCGCTGAATTTAAG\n",
        "```\n",
        "\n",
        "Named Symbiodiniaceae sequences from SymPortal\n",
        "```\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Symbiodinium;A4n\n",
        "AATGGCCTCTTGAACGTGCATTGTCTCTTGGGATATGCCTGAGAGCATGTCTGCTTCAGTGCTTCTACTTTCATTTTCTGCTGCTCTTGTTATCGGGAGCAGTGTTGCTGCATGCTTCTGCAAAAGGCACTGGCATGCTAAGTATCAAGTTTTGCTTGCTGTTCTGACTGATCAACATCTCATGTCGTTTCAGTTGGTGAAACAAAAGCTGAAGTGTGTTCTTAACACTTCCTA\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;C3br\n",
        "AATGGCCTCCTGAACGTGCGTTGCACTCTTGGGATTTCCTGAGAGTATGTCTGCTTCAGTGCTTAACTTGCCCCAACTTTGCAAGCAGGATGTGTTTCTGCCTTGCGTTCTTATGAGCTATTGCCCTCTGCGCCAATGGCTTGTTAATTGCTTGGTTCTTGCAAAATGCTTTGCGCGCTGTTATTCAAGCTTCTACCTTCAAGTAGTTTTACTTGAGTGACGCTGCTCATGCTTGCAACTGCTGGGATGCAGGTGCATGCCTCTA\n",
        "```\n",
        "\n",
        "ITS2 sequences from Symbiodiniaceae via Symbiodatabaceae\n",
        "```\n",
        "# did not have a match > 96 identity and > 200 bp to any SymPortal named sequence\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Fugacium;F5_AF180130\n",
        "AACCAATGGCCTCCTGAACGTACGTTGCACTCTGGGATTCCTGAGAGTATGTAGCTTCAGTGCTTAGCTTGCCCACTTGCGGATAGATTTTGTTTCTGTCTTGCGCCCCGTGAGCCATTGAAACTCTAGTCAATGGCTTATTGAATGTCGGTCTTGCAAAAGCTTTGCGCGATGCTATTCAAGATTCCACTGAAATGGTATTTCTTGAGTGACGCTGCTTATGCTTGCAACTGCGGATGCTAGCGCATGCCTGTAGCATGAAGTCAGACAAGTGA\n",
        "\n",
        "# had 100% match to C56 SymPortal named sequence\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;C56_AY589760_C56\n",
        "AACCAATGGCCTCCTGAACGTACGTTGCACTCTCGGGTTTTCCTGAGAGTATGTCTGCTTCAGTGCTTAGCTTTCTCAACTTTGCAAAGCAATTCCCTTATTGCCTTGCGTTCCTATGAGCCATTGACAATCATGGTCCATGGCTTGTTGACCAACCGGTTCCTGCAAAGCTTTGCGCGCTGTTGTTCAAGCTTTTCTGCTTTGAGTGACGCTGCTGACGCTTGCAACCGCCGGGGTGCCCATGCACATCTCTAGCATGAAGTCAGACAAGCAA\n",
        "\n",
        "# had 99.2% match to SymPortal's F4.8 and also 99.2% match to SymPortal's F4.8c named sequences (only 1-2 matches are appended; here the second match is appened because identity is not lower than the first match, but the bit score is lower)\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;clade_Fr4;F4.8c_AJ291527_F4.8_99.2_F4.8c_99.2\n",
        "AACCAATGGCCTCCTGAACGTACGTTGCACTCTCGGGATTTCCTGAGAGTATGTCTGCTTCAGTGCTTAGCTTTCTCAACCTTGCAAAGCAAAGTCTTTTCTTTGCTGTTGCGCTCCTGTGGGCCATTGACAATCATGGTCARTGGCTTGTTGACTAACCGGTTCCTGCAAAGCTTTGCGCGCTGTTGTTCAAGCTTCTCTGCTCTGAGTGACGCTGCTCACGCTTGCAACCGCCGGGGTGCTGAAGCACAACTCTAGCATGAAGTCAGACAAGTGA\n",
        "```\n",
        "\n",
        "Symbiodiniaceae sequences specific for our dataset generated by SymPortal run:\n",
        "```\n",
        "# internal id 5882_C\n",
        "# has 99.6% identity to C1ae\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;5882_C_id99.6_C1ae\n",
        "AATGGCCTCCTGAACGTGCGTTGCACTCTTGGGATTTCCTGAGAGTATGTCTGCTTCAGTGCTTAACTTGCCCCAACTTTGCAAGCAGATGTGTTTCTGCCTTGCGTTCTTATGAGCTATTGCCCTCTGAGCCAATGGCTTGTTAATTGCTTGGTTCTTGCAAAATGCTTTGCGCGCTGTTATTCAGGTTTCTACCTTCTTGGTTTTACTTGAGTGACGCTGCTCATGCTTGCAACCGCTGGGATGCAGGTGCATGCCTCTA\n",
        "\n",
        "# internal id 6059_C\n",
        "# has 99.6% identity to ITS2 sequence from Symbiodatabaceae C21_AY239372|C3d_AF499792 (GB accession numbers AY239372, AF499792)\n",
        "# which in turn has 97.2% identity to SymPortal's C21\n",
        ">Eukaryota;Dinoflagellata;Dinophyceae;Suessiales;Symbiodiniaceae;Cladocopium;6059_C_id99.6_C21_AY239372|C3d_AF499792_C40c_97.2_C21\n",
        "AATGGCCTCCTGAACGTGCGTTGCACTCTTGGGATTTCCTGAGAGTATGTCTGCTTCAGTGCTTAACTTGCCCCAACTTTGCAAGCATTTCTGCCTTGCGTTCTTATGAGCCATTGCCCTCTGAGCCAATGGCTTGTTAATTGCTTGGTTCTTGCAAAATGCTTTGCGCGCTGTTATTCAAGTTTCTACCTTCGTGGTTTTACTTGAGTGACGCTGCTCATGCTTGCAACCGCTGGGATGCAGGTGCATGCCTCTA\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "wShmK1AIIGc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign Taxonomy with DADA2"
      ],
      "metadata": {
        "id": "sq6WOtKKSUB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for assigning taxonomy was taken and modified from https://benjjneb.github.io/dada2/assign.html\n",
        "\n",
        "Files used:\n",
        "*   `ASVs_counts_curated.txt` - table with counts of ASVs for each sample\n",
        "*   `ASVs_curated.fasta`  - fasta with curated ASVs\n",
        "*   `ITS2db_SymPortaldb_full.fasta` - database\n",
        "\n",
        "Code in `Symbiodiniaceae_dada2_assign_taxonomy.Rmd`"
      ],
      "metadata": {
        "id": "EdknqXtRe4r2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "> seqkit stat inputs/ITS2db_SymPortaldb_full.fasta\n",
        "file                                  format  type  num_seqs    sum_len  min_len  avg_len  max_len\n",
        "inputs/ITS2db_SymPortaldb_full.fasta  FASTA   DNA     18,139  4,379,866      150    241.5    1,572\n",
        "\n",
        "> seqkit stat inputs/ASVs_curated.fasta\n",
        "file                       format  type  num_seqs  sum_len  min_len  avg_len  max_len\n",
        "inputs/ASVs_curated.fasta  FASTA   DNA        980  291,538      201    297.5      538\n",
        "```"
      ],
      "metadata": {
        "id": "K5CNicKqYWP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outputs from SymPortal running and dada2"
      ],
      "metadata": {
        "id": "pvrYzoZAYpRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs from dada2 taxonomy assignment:\n",
        "\n",
        "`ASV_Symbiodiniaceae_filtered.fasta`,\n",
        "`ASV_Symbiodiniaceae_prelulu.fasta`,\n",
        "`ASV_Symbiodiniaceae_unfiltered.fasta` - fasta files with ASV sequences\n",
        "\n",
        "`ASV_Symbiodiniaceae_new_names_filtered.fasta` - fasta file with Symbiodiniaceae ASVs renamed according to their closest identity match.\n",
        "\n",
        "`ASVs_counts_Dinoflagellata.tsv`\n",
        "`ASV_counts_Dino_t.tsv`,\n",
        "`ASVs_counts_Sym_prelulu.tsv`,\n",
        "`ASV_counts_Symbiodiniaceae_aliquotes.tsv`,\n",
        "`ASV_counts_Symbiodiniaceae_unfiltered.tsv`,\n",
        "`ASV_counts_Symbiodiniaceae_filtered.tsv`- ASV counts tables, rows are ASVs, columns are samples, cells are counts.\n",
        "\n",
        "`ASV_counts_Symbiodiniaceae_filtered_t.tsv`,\n",
        "`ASV_counts_Symbiodiniaceae_unfiltered_t.tsv`,\n",
        "`ASV_counts_Symbiodiniaceae_aliquotes_t.tsv` - ASV counts transposed tables, rows are samples, columns are ASVs, cells are counts.\n",
        "\n",
        "`ASV_tax_Dinoflagellata.tsv`,\n",
        "`ASV_tax_Symbiodiniaceae_filtered.tsv`,\n",
        "`ASV_tax_Symbiodiniaceae_prelulu`,\n",
        "`ASV_tax_Symbiodiniaceae_prelulu.tsv`,\n",
        "`ASV_tax_Symbiodiniaceae_unfiltered.tsv` - taxonomy tables, rows are ASVs, columns are Phylum\tClass\tOrder\tFamily\tGenus\tSpecies.\n",
        "\n",
        "\n",
        "`DIV_counts_filtered.tsv`,\n",
        "`DIV_counts_filtered_t.tsv`,\n",
        "`DIV_counts_unfiltered.txt`,\n",
        "`DIV_counts_unfiltered_t.txt` - DIV counts\n",
        "\n",
        "`DIV_filtered.fasta` - DIV filtered sequences\n",
        "\n",
        "`DIV_tax_filtered.tsv`,\n",
        "`DIV_tax_unfiltered.tsv` - DIV tax tables, genera are \"clade A\", \"clade B\"... etc\n",
        "\n",
        "`dada2_taxa_genera.txt` - raw tax table, each row is ASV sequence\n",
        "`dada2_taxa_genera_pre_lulu.txt` - raw tax table of ASVs before lulu curation, each row is ASV sequence\n",
        "\n",
        "`full_stats.csv` - number of counts before/after filtering and curation for SymPortal and dada2."
      ],
      "metadata": {
        "id": "lsWNvFb7Upd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The further steps of the analysis are written in R Markdown documents `barplots_heatmap/Symbiodiniaceae_barplots_heatmap.Rmd`, `phyloseq/Symbiodiniaceae_tree.Rmd`,  `phyloseq/Symbiodiniaceae_phyloseq.Rmd`, and `symportal_phyloseq/Symbiodiniaceae_SymPortal_phyloseq.Rmd`"
      ],
      "metadata": {
        "id": "t5ZCOCOwdE0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Host id"
      ],
      "metadata": {
        "id": "REESj0EzPCYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building maximum-likelihood phylogenetic tree for COI of Foraminifera.\n",
        "`Foraminifera_COI_aln.fasta` and `Foraminifera_18S_aln.fasta` was generated and visually acessed with Muscle 5.1 in Geneious Prime and then ecported using File -> Export -> Documents in .fasta format.\n",
        "\n",
        "Next steps are on `Symbiodiniaceae_host_id_tree.Rmd`"
      ],
      "metadata": {
        "id": "hhF212bNcdvh"
      }
    }
  ]
}